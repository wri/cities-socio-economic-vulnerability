---
title: "Brazil Censo 2010"
output:
  html_notebook: 
    code_folding: hide
---

```{r setup, include=FALSE}
rm(list=ls()) # Clear environment
knitr::opts_chunk$set(echo = TRUE, eval=FALSE)
```
The goal of this tutorial is to enable anyone to replicate the data processing for Brazil's socioeconomic data dashboard, developed by the World Resources Institute - Cities.

# Introduction
The 2010 Brazilian Census, conducted by the Brazilian Institute of Geography and Statistics (IBGE), represents a vital source of demographic data, offering a comprehensive snapshot of Brazil's population and housing conditions as of August 1, 2010. This census was designed to gather detailed information on various socio-economic factors across the country, making it an invaluable resource for researchers, policymakers, and analysts.

## Data Structure Overview:
The 2010 Brazilian Census data is organized into several key components, each designed to facilitate detailed analysis and reporting. Understanding this structure is essential for effectively navigating and utilizing the data.

* Microdata: The census microdata provides the most granular level of data disaggregation. It is encoded in numerical formats that preserve statistical confidentiality, ensuring the anonymity of individuals. These files are suitable for users with statistical software proficiency to analyze and cross-reference across different geographic levels. This allows for the creation of custom tabulations and detailed demographic studies.

* Variable Documentation: Accompanying each dataset is comprehensive documentation that details the variables, their corresponding codes, and brief descriptions.

* Geographical Levels
The census data is available at various geographical levels:

  * Federation Units (uf): Brazil is divided into 27 Federation Units, comprising 26 states and the Federal District (Brasília). This is the highest administrative level in the census framework.

  * Regions: The Federation Units are grouped into five geographic regions: North; Northeast; Central-West; Southeast; South

  * Mesoregions: Brazil is further divided into 137 mesoregions, which serve as intermediate geographical units between states and municipalities. Mesoregions are designed to group municipalities that share similar socio-economic characteristics and geographical features.

  * Microregions: These mesoregions are subdivided into 558 microregions. Microregions typically consist of smaller groups of municipalities that are more homogenous in terms of economic activities, cultural aspects, and demographic profiles.

  * Municipalities: Brazil has 5,565 municipalities, which represent the basic local administrative units for data collection.

  * Weighting Areas (wa). The census utilized 10,184 weighting areas, designed to include a specific number of housing units. These areas are classified as either urban or rural, allowing for detailed demographic analysis.

  * Census Tract (ct). Census tracts are the smallest territorial units used in the census, consisting of a contiguous area entirely contained within either an urban or rural setting. These tracts are designed to be of manageable size for efficient survey operations. In the 2010 Census, there are 179,812 census tracts across Brazil.


# I. Downloading tabular data `bra_00_data.R`

First, visit the [IBGE´s Microdata section](https://www.ibge.gov.br/en/statistics/social/labor/18391-2010-population-census.html?lang=en-GB&t=microdados). On this page, you can find the census data organized by Federation Units, with each zip file containing four text files (*.txt*):

* Amostra_Pessoas (population data),
* Amostra_Domicilios (dwellings),
* Amostra_Emigracao (emigration), and
* Amostra_Mortalidade (deaths)

Fortunately, Rafael Pereira has developed a great R script to download and organize all the microdata. You can find Pereira’s code [here](https://github.com/rafapereirabr/Brazilian_Census/blob/master/Download%20CensoBrasil%202010_git.R#L27). We will modify this script to download only the dwelling and population files, as these are the primary datasets we need.

Since these files contain microdata for the entire country, they are quite large and can be difficult to manipulate. To make handling the data easier, we will modify the  Rafael Pereira’s work to save the data in folders by Federation Unit and as *CSV* files by municipality.

Before running the script, be aware that it may take approximately 1 hour to complete.

**1. Set up**. In this step, we’ll begin by clearing the R environment to ensure that no residual objects interfere with our work. We then load essential libraries for data manipulation and set the working directory to organize our files effectively.

Next, we create subdirectories to store the raw data files in their original **.txt** format and their processed **.csv** counterparts. We proceed by defining a list of abbreviations for Brazil's Federation Units (states) and specifying the FTP path where the census microdata is hosted. The code then loops through each Federation Unit, downloading the corresponding zip files and extracting the contents. 

```{r}
rm(list=ls()) # Clear environment
library(data.table) # to manipulate data frames (fread and fwrite are ultrafast for reading and writing CSV files)
library(readr) #fast read of fixed witdh files
library(readxl) # read excel spreadsheets
library(zip)
options(scipen=999) # disable scientific notation

setwd("01_data/01_raw") # set working Directory
# Create subdirectories to save files
dir.create(file.path(".", "dados_txt2010"))
dir.create(file.path(".", "dados_csv2010"))

destfolder <- "./dados_txt2010/"
# List of Federation Unit abbreviations
UFlist <- c("AC","AL","AM","AP","BA","CE","DF","ES","GO","MA","MG","MS","MT","PA","PB","PE","PI","PR","RJ","RN","RO","RR","RS","SC","SE","SP1","SP2_RM","TO")
ftppath <- "ftp://ftp.ibge.gov.br/Censos/Censo_Demografico_2010/Resultados_Gerais_da_Amostra/Microdados/"

tf <- tempfile()
td <- tempdir()

for (i in UFlist){
  tf <- paste0(ftppath, i, ".zip")
  td <- paste0("./dados_txt2010/", i, ".zip")
  print(i)
  download.file(tf, td, mode="wb")
}

# Unzip all files
filenames <- list.files("./dados_txt2010", pattern=".zip", full.names=TRUE)
lapply(filenames, unzip, exdir = "./dados_txt2010")
```

**2. Download Census Documentation**. Then, we focus on obtaining and preparing the essential documentation that accompanies the 2010 Brazilian Census microdata. This documentation includes detailed layouts of the variables contained in the datasets, which are crucial for understanding the structure and meaning of the data. We begin by downloading the documentation files from the IBGE’s FTP server and extracting the relevant Excel files that describe the variable formats for both dwellings and population data. 

```{r eval=FALSE, include=FALSE}
file_url <- "ftp://ftp.ibge.gov.br/Censos/Censo_Demografico_2010/Resultados_Gerais_da_Amostra/Microdados/Documentacao.zip"
download.file(file_url, "Documentacao.zip", mode="wb")
zip::unzip("Documentacao.zip", exdir="documentacao2010", junkpaths=T)

# Open variable layouts from Excel file
dic_dom <- read_excel("./documentacao2010/Layout_microdados_Amostra.xls", sheet = 1, skip = 1)
dic_pes <- read_excel("./documentacao2010/Layout_microdados_Amostra.xls", sheet = 2, skip = 1)

# Convert to data table
setDT(dic_dom)
setDT(dic_pes)

# Function to compute variable widths
computeWidth <- function(dataset) {
  dataset[is.na(DEC), DEC := 0] # Convert NA to 0
  dataset[, width := INT + DEC] # Create width variable 
  setnames(dataset, colnames(dataset)[which(colnames(dataset) == "POSIÇÃO INICIAL")], "pos.ini") # Rename initial position
  setnames(dataset, colnames(dataset)[which(colnames(dataset) == "POSIÇÃO FINAL")], "pos.fin") # Rename final position
}

# Apply function to variable layouts
lapply(list(dic_dom, dic_pes), computeWidth)

```

**3. Downloading the Data (dwellings and population)**
Using a modified script based on Rafael Pereira’s work, we downloaded the microdata files for each Federation Unit, focusing on the datasets for dwellings and population. These large, raw text files were then processed, split by municipality, and saved as CSV files for easier handling in subsequent analysis. 
```{r}
# Dwellings
dir.create(file.path("dados_csv2010", "dom"), recursive = TRUE)


# List with all Dwellings files
data_files <- list.files(path = "./dados_txt2010",
                         recursive = TRUE,
                         pattern = "Dom",
                         full.names = TRUE)

# Create function to read Dwellings files
readDOM <- function(f) {
  cat(f, "\n")
  data <- read_fwf(f,
                   fwf_positions(dput(dic_dom[, pos.ini]),
                                 dput(dic_dom[, pos.fin]),
                                 col_names = dput(dic_dom[, VAR])),
                   progress = interactive())
  return(data)
}

# Loop through each file and save by V0001 (state) and V0002 (municipality)
for (f in data_files) {
  temp <- readDOM(f)
  setDT(temp)
  
  # Update decimals in the data
  var.decimals <- dic_dom[DEC > 0, ]  # identify variables with decimals to update
  var.decimals <- var.decimals[, .(VAR, DEC)]
  var <- var.decimals$VAR  # list of variables to update decimals
  
  for (j in seq_along(var)) {
    set(temp, j = var[j], value = as.numeric(temp[[var[j]]]) / 10^var.decimals[j, DEC])
  }
  
  # Save each subset of data by V0001 and V0002
  unique_states <- unique(temp$V0001)
  for (state in unique_states) {
    state_data <- temp[V0001 == state]
    
    # Create directory for the state if it doesn't exist
    state_dir <- file.path("dados_csv2010", "dom", as.character(state))
    if (!dir.exists(state_dir)) {
      dir.create(state_dir, recursive = TRUE)
    }
    
    unique_municipalities <- unique(state_data$V0002)
    for (municipality in unique_municipalities) {
      municipality_data <- state_data[V0002 == municipality]
      fwrite(municipality_data, file = file.path(state_dir, paste0(municipality, ".csv")))
    }
  }
}

rm(temp, readDOM); gc()


## Population
# Create the output directory if it does not exist
dir.create(file.path("dados_csv2010", "pes"), recursive = TRUE)

# List with all Household files
data_files <- list.files(path = "./dados_txt2010",
                         recursive = TRUE,
                         pattern = "Pes",
                         full.names = TRUE)

# Prepare documentation to update decimals in the data
var.decimals <- dic_pes[DEC > 0, ]  # identify variables with decimals to update
var.decimals <- var.decimals[, .(VAR, DEC)]
var <- var.decimals$VAR  # list of variables to update decimals


# Loop through each file and process
for (i in seq_along(data_files)) {
  
  # Select state file
  file <- data_files[i]
  
  # Read data into temp
  temp <- read_fwf(file,
                   fwf_positions(dput(dic_pes[, pos.ini]),
                                 dput(dic_pes[, pos.fin]),
                                 col_names = dput(dic_pes[, VAR])),
                   progress = interactive())
  setDT(temp)  # Set as Data Table 
  
  # Update decimals in the data
  for (j in seq_along(var)) {
    set(temp, j = var[j], value = as.numeric(temp[[var[j]]]) / 10^var.decimals[j, DEC])
  }
  
  # Save each subset of data by V0001 and V0002
  unique_states <- unique(temp$V0001)
  for (state in unique_states) {
    state_data <- temp[V0001 == state]
    
    # Create directory for the state if it doesn't exist
    state_dir <- file.path("dados_csv2010", "pes", as.character(state))
    if (!dir.exists(state_dir)) {
      dir.create(state_dir, recursive = TRUE)
    }
    
    unique_municipalities <- unique(state_data$V0002)
    for (municipality in unique_municipalities) {
      municipality_data <- state_data[V0002 == municipality]
      fwrite(municipality_data, file = file.path(state_dir, paste0(municipality, ".csv")))
    }
  }
  
  cat("saving", i, "out of", length(data_files), file, "\n")  # Update status of the loop
  rm(temp); gc()
}
```

**Final Structure**. At the end of running this code, you will have three folders in your main directory:

* `documentacao2010` –  documentation and variable layouts.
* `dados_txt2010` – original text files downloaded from IBGE.
* `dados_csv2010` – CSV files, organized by Federation Unit and municipality.

Before moving on to the next section, which covers recoding, it's crucial to understand the limitations of the downloaded data. While you can group the data by most geographic levels directly, there are exceptions. The datasets include a column for metropolitan codes, but all values are set to 0. To address this, an additional step will be required to merge these data with geojson files that contain the correct metropolitan area identifiers. Similarly, handling census tract data requires a special approach ^[For more information on this level, please refer to *Base de informações do Censo Demográfico 2010: Resultados do
Universo por setor censitário*, IBGE, 2011], which we will detail in the next subsection.

## Census Tract

**1. Downloading and Grouping Files**. We manually download the census tract data from the [BGE website.] 
(https://www.ibge.gov.br/estatisticas/multidominio/cultura-recreacao-e-esporte/9662-censo-demografico-2010.html?edicao=10410).  The site provides *zip* files for each Federation Unit (UF), with each zip file containing two folders of *csv* and *xls* files. Each file corresponds to one of 26 different topics.

After downloading the zip files, we unzip them and save all the files into a new directory:  `01_data/01_raw/sc/00_unzip`. To organize these files, we use the following R code to:

1. Identify and Copy XLS Files: This code scans the source directory for all xls files and copies them to a new directory named `01_xls`.

2. Group Files by Prefix: It then processes the copied *xls* files, extracting the prefix from each filename (the part before the first underscore). Based on this prefix, it creates subdirectories in a new destination directory named `01_xls_topic` and moves each file into its corresponding subdirectory.

Then, with R we made a code to: 1) List all *xls* files in the new source directory. For each file, it extracts the filename prefix (the part before the first underscore). 2) Create a subdirectory within the destination directory based on this prefix, if it does not exist and 3) copy each file into its respective sub directory, named according to its prefix

```{r}
# 1. Identify and Copy XLS Files 

# Define the source and destination directories
source_dir <- "01_data/01_raw/sc/00_unzip"  # Source directory containing .xls files
dest_dir <- "01_data/01_raw/sc/01_xls"      # Destination directory for copied .xls files

# Create the destination directory if it doesn't exist
if (!dir.exists(dest_dir)) {
  dir.create(dest_dir, recursive = TRUE)
}

# List all .xls files in the source directory and its subdirectories
xls_files <- list.files(source_dir, pattern = "\\.xls$", recursive = TRUE, full.names = TRUE)

# Copy the .xls files to the destination directory
file.copy(xls_files, file.path(dest_dir, basename(xls_files)), overwrite = TRUE)


# 2. Grouping Files by Prefix 

# Define source and destination directories
source_dir <- "01_data/01_raw/sc/01_xls"           # Directory with .xls files
dest_dir <- "01_data/01_raw/sc/01_xls_topic"       # Directory for grouped .xls files

# List all .xls files in the source directory
xls_files <- list.files(source_dir, pattern = "\\.xls$", full.names = TRUE)

# Loop through each .xls file
for (file in xls_files) {
  # Extract the filename without the path
  filename <- basename(file)
  
  # Extract the prefix from the filename (assumed to be before the first underscore)
  prefix <- sub("_.*", "", filename)
  
  # Define the specific destination directory based on the prefix
  specific_dest_dir <- file.path(dest_dir, prefix)
  
  # Create the specific destination directory if it doesn't exist
  if (!dir.exists(specific_dest_dir)) {
    dir.create(specific_dest_dir, recursive = TRUE)
  }
  
  # Copy the file to the specific destination directory
  file.copy(file, file.path(specific_dest_dir, filename), overwrite = TRUE)
}
```

**Final Structure**
By the end of this code execution, all files will be organized into topic-specific subdirectories within `dest_dir`.

# II. Recoding `bra_01_recoding`
In this section, we focus on the recoding process, which involves transforming the data to create binary variables. This transformation simplifies the calculation of variables.

We will start by recoding the dwellings data, followed by the population data for all levels. Finally, we will apply the same recoding process to the census tract information.

The processing code first reads the raw *csv* files and applies the transform_data function to recode the original variables. It then splits the transformed data by `wa_id` and organizes the results into folders based on reg_code. Finally, the processed files are saved in the `output_folder`, adhering to a structured naming convention and directory organization.

```{r}
## Dwellings
transform_data <- function(df) {
  df %>%
    filter(
          V4002 %in% c(11, 12, 13, 14, 15)) %>% # Keep dwg that are habitable (Casa, Casa de vila ou em condomínio, Apartamento, Habitação em: casa de cômodos, cortiço ou cabeça de porco, Oca ou maloca)
     mutate(
       # Weight
       weight = V0010,
       # Geoids
       uf_code = V0001,
       mun_code = V0002,
       wa_id = V0011,
       reg_code = V1001,
       mesoreg_code = V1002,
       microreg_code = V1003,
       sector = V1005,
       # Urbanized
       urban = ifelse(is.na(V1006), NA, ifelse(V1006 == 1, 1, 0)),
       rural = ifelse(is.na(V1006), NA, ifelse(V1006 == 2, 1, 0)),
       
       # Income
       income = V6529,
       pcincome = V6531,
       
       # Ownership
       owned = ifelse(V0201 == 1 | V0201 == 2, 1, 0),
       rented = ifelse(V0201 == 3, 1, 0),
       rv = V2011,
       
       loaned = ifelse(V0201 == 4 | V0201 == 5, 1, 0),
       other = ifelse(V0201 == 6, 1, 0),
       
       # Quality
       ## Walls material
       walls_masonry = ifelse(is.na(V0202) | V0202 == 9, NA, ifelse(V0202 == 1 | V0202 == 2, 1, 0)),
       walls_wood = ifelse(is.na(V0202) | V0202 == 9, NA, ifelse(V0202 == 3 | V0202 == 6, 1, 0)),
       walls_rearth = ifelse(is.na(V0202) | V0202 == 9, NA, ifelse(V0202 == 4 | V0202 == 5, 1, 0)),
       walls_straw = ifelse(is.na(V0202) | V0202 == 9, NA, ifelse(V0202 == 7, 1, 0)),
       walls_other = ifelse(is.na(V0202) | V0202 == 9, NA, ifelse(V0202 == 8, 1, 0)),
       nwalls = ifelse(is.na(V0202), NA, ifelse(V0202 == 9, 1, 0)),
       
       ## Rooms and bedrooms
       `1rm` = ifelse(is.na(V0203), NA, ifelse(V0203 == 1, 1, 0)),
       `2rms` = ifelse(is.na(V0203), NA, ifelse(V0203 == 2, 1, 0)),
       `3mrms` = ifelse(is.na(V0203), NA, ifelse(V0203 >= 3, 1, 0)),
       rm_occup = V6203, 
       `1bedrm` = ifelse(is.na(V0204), NA, ifelse(V0204 == 1, 1, 0)),
       `2mbedrms` = ifelse(is.na(V0204), NA, ifelse(V0204 >= 2, 1, 0)),  
       bedrm_occup = V6204,      
       
       ### novc : no overcrowd
       novc = ifelse(is.na(V6204), NA, ifelse(V6204 < 2.5, 1, 0)),
       
       ## Bathrooms  
       bthrm = ifelse(is.na(V0205), NA, ifelse(V0205 > 1, 1, 0)),
       toilet = ifelse(is.na(V0206), NA, ifelse(V0206 == 1, 1, 0)), # This variable has values only if V0205=0
       obdhnb = ifelse(is.na(V0205) | is.na(V0206), NA, ifelse(V0205 == 0 & V0206 == 2, 1, 0)),
       
       # Basic services
       ## Sewage
       ss_sewage = ifelse(is.na(V0207), NA, ifelse(V0207 == 1, 1, 0)),
       ss_pit = ifelse(is.na(V0207), NA, ifelse(V0207 == 2 | V0207 == 3, 1, 0)),
       ss_ditch = ifelse(is.na(V0207), NA, ifelse(V0207 == 4, 1, 0)),
       ss_river = ifelse(is.na(V0207), NA, ifelse(V0207 == 5, 1, 0)),
       ss_other = ifelse(is.na(V0207), NA, ifelse(V0207 == 6, 1, 0)),
       
       ## Water
       water_netwk = ifelse(is.na(V0208), NA, ifelse(V0208 == 1, 1, 0)),
       water_well = ifelse(is.na(V0208), NA, ifelse(V0208 %in% c(2,3), 1, 0)),
       water_truck = ifelse(is.na(V0208), NA, ifelse(V0208 == 4, 1, 0)),
       water_rain = ifelse(is.na(V0208), NA, ifelse(V0208 %in% c(5,6), 1, 0)),                        
       water_river = ifelse(is.na(V0208), NA, ifelse(V0208 == 7, 1, 0)),
       water_other = ifelse(is.na(V0208), NA, ifelse(V0208 > 7, 1, 0)),
       
       ## Garbage
       garbage_clngsvcs = ifelse(is.na(V0210), NA, ifelse(V0210 %in% c(1,2), 1, 0)),
       garbage_burnt = ifelse(is.na(V0210), NA, ifelse(V0210 == 3, 1, 0)),
       garbage_buried = ifelse(is.na(V0210), NA, ifelse(V0210 == 4, 1, 0)),
       garbage_throwed = ifelse(is.na(V0210), NA, ifelse(V0210 %in% c(5,6), 1, 0)),
       garbage_other = ifelse(is.na(V0210), NA, ifelse(V0210 == 7, 1, 0)),
       
       ## Electricity
       elect = ifelse(is.na(V0211), NA, ifelse(V0211 %in% c(1,2), 1, 0)),
       
       ## Internet
       inet = ifelse(is.na(V0220), NA, ifelse(V0220 == 1, 1, 0)),
       
       # Goods
       tv = ifelse(is.na(V0214), NA, ifelse(V0214 == 1, 1, 0)),
       wm = ifelse(is.na(V0215), NA, ifelse(V0215 == 1, 1, 0)),
       refri = ifelse(is.na(V0216), NA, ifelse(V0216 == 1, 1, 0)),
       cp = ifelse(is.na(V0217), NA, ifelse(V0217 == 1, 1, 0)),
       cmp = ifelse(is.na(V0219), NA, ifelse(V0219 == 1, 1, 0)),
       mtrcl = ifelse(is.na(V0221), NA, ifelse(V0221 == 1, 1, 0)),
       car = ifelse(is.na(V0222), NA, ifelse(V0222 == 1, 1, 0))
    ) %>%
    select(
      # Weight
      weight,
      #Geoids
      uf_code, mun_code, wa_id,
      reg_code, mesoreg_code, microreg_code, 
      # Urbanized
      urban, rural,
      # Income
      income, pcincome,
      # Alquiler
      rv,
      # Recoded
      owned, rented, loaned, other,
      walls_masonry, walls_wood, walls_rearth, walls_straw, walls_other, nwalls,
      `1rm`, `2rms`, `3mrms`, rm_occup, `1bedrm`, `2mbedrms`, bedrm_occup,
      novc,
      bthrm, toilet, obdhnb,
      ss_sewage, ss_pit, ss_ditch, ss_river, ss_other,
      water_netwk, water_well, water_truck, water_rain, water_river, water_other,
      garbage_clngsvcs, garbage_burnt, garbage_buried, garbage_throwed, garbage_other,
      elect,
      inet,
      tv, wm, refri, cp, cmp, mtrcl, car)
}


# File paths
input_folder <- "01_data/01_raw/dados_csv2010/dom"
dir.create("01_data/02_proc/01_recoded/dwg/")
output_folder <- "01_data/02_proc/01_recoded/dwg/"

# Get list of uf folders
uf_folders <- list.dirs(input_folder, recursive = FALSE)

# Process each uf folder
for (uf_folder in uf_folders) {
  uf_name <- basename(uf_folder)
  
  # Get list of municipality CSV files
  file_list <- list.files(uf_folder, pattern = "\\.csv$", full.names = TRUE)
  
  # Process each file
  for (file in file_list) {
    df <- fread(file, showProgress = FALSE)
    
    # Extract wa_id from the file name
    wa_id <- tools::file_path_sans_ext(basename(file))
    
    # Transform the data
    df_transformed <- transform_data(df)
    
    # Split the transformed data by wa_id
    wa_id_split <- split(df_transformed, df_transformed$wa_id)
    
    # Process each split by wa_id
    for (wa_id_name in names(wa_id_split)) {
      df_wa_id <- wa_id_split[[wa_id_name]]
      
      # Get unique reg_codes from the transformed data
      reg_codes <- unique(df_wa_id$reg_code)
      
      # Create a folder for each reg_code and save the corresponding files
      for (reg_code in reg_codes) {
        # Define the output directory for the reg_code
        reg_code_folder <- file.path(output_folder, as.character(reg_code))
        if (!dir.exists(reg_code_folder)) {
          dir.create(reg_code_folder, recursive = TRUE)
        }
        
        # Construct the new file name and output path
        new_file_name <- paste0(wa_id_name, ".csv")
        output_file <- file.path(reg_code_folder, new_file_name)
        
        # Save the transformed data to the new folder
        fwrite(df_wa_id, output_file)
      }
    }
  }
}
```

```{r}
## Pop
transform_data <- function(df) {
  df %>%
    mutate(
      # Weight
      weight = V0010,
      # Geoids
      uf_code = V0001,
      mun_code = V0002,
      wa_id = V0011,
      reg_code = V1001,
      mesoreg_code = V1002,
      microreg_code = V1003,
      # Urban
      urban = ifelse(is.na(V1006), NA, ifelse(V1006 == 1, 1, 0)),
      rural = ifelse(is.na(V1006), NA, ifelse(V1006 == 2, 1, 0)),
      # Sex
      tot_m = ifelse(V0601 == 1, 1, 0),
      tot_f = ifelse(V0601 == 2, 1, 0),
      
      # Age groups
      `0to5` = ifelse(V6036 >= 0 & V6036 <= 5, 1, 0),
      `0to5_f` = ifelse(`0to5` == 1 & V0601 == 2, 1, 0),
      `0to5_m` = ifelse(`0to5` == 1 & V0601 == 1, 1, 0),
      
      `6to11` = ifelse(V6036 >= 6 & V6036 <= 11, 1, 0),
      `6to11_f` = ifelse(`6to11` == 1 & V0601 == 2, 1, 0),
      `6to11_m` = ifelse(`6to11` == 1 & V0601 == 1, 1, 0),
      
      `12to17` = ifelse(V6036 >= 12 & V6036 <= 17, 1, 0),
      `12to17_f` = ifelse(`12to17` == 1 & V0601 == 2, 1, 0),
      `12to17_m` = ifelse(`12to17` == 1 & V0601 == 1, 1, 0),
      
      `18to24` = ifelse(V6036 >= 18 & V6036 <= 24, 1, 0),
      `18to24_f` = ifelse(`18to24` == 1 & V0601 == 2, 1, 0),
      `18to24_m` = ifelse(`18to24` == 1 & V0601 == 1, 1, 0),
      
      `25to64` = ifelse(V6036 >= 25 & V6036 <= 64, 1, 0),
      `25to64_f` = ifelse(`25to64` == 1 & V0601 == 2, 1, 0),
      `25to64_m` = ifelse(`25to64` == 1 & V0601 == 1, 1, 0),
      
      `65m` = ifelse(V6036 >= 65, 1, 0),
      `65m_f` = ifelse(`65m` == 1 & V0601 == 2, 1, 0),
      `65m_m` = ifelse(`65m` == 1 & V0601 == 1, 1, 0),
      
      tdr = ifelse(V6036 >= 15 & V6036 <= 64, 0, 1),
      chddr = ifelse(V6036 < 15, 1, ifelse(V6036 >= 65, NA, 0)),
      olddr = ifelse(V6036 >= 65, 1, ifelse(V6036 >= 15 & V6036 <= 64, 0, NA)),
      
      # Race
      wht = ifelse(V0606 == 1, 1, 0),
      wht_f = ifelse(V0606 == 1 & V0601 == 2, 1, 0),
      wht_m = ifelse(V0606 == 1 & V0601 == 1, 1, 0),
      
      black = ifelse(V0606 == 2, 1, 0),
      black_f = ifelse(V0606 == 2 & V0601 == 2, 1, 0),
      black_m = ifelse(V0606 == 2 & V0601 == 1, 1, 0),
      
      yellow = ifelse(V0606 == 3, 1, 0),
      yellow_f = ifelse(V0606 == 3 & V0601 == 2, 1, 0),
      yellow_m = ifelse(V0606 == 3 & V0601 == 1, 1, 0),
      
      brown = ifelse(V0606 == 4, 1, 0),
      brown_f = ifelse(V0606 == 4 & V0601 == 2, 1, 0),
      brown_m = ifelse(V0606 == 4 & V0601 == 1, 1, 0),
      
      indig = ifelse(V0606 == 5, 1, 0),
      indig_f = ifelse(V0606 == 5 & V0601 == 2, 1, 0),
      indig_m = ifelse(V0606 == 5 & V0601 == 1, 1, 0),
      
      # Disability
      ## Seeing
      disab_vis = ifelse(is.na(V0614), NA, ifelse(V0614 == 4, 0, 1)),
      disab_vis_f = ifelse(V0601 == 2, ifelse(V0614 == 4, 0, 1), NA),
      disab_vis_m = ifelse(V0601 == 1, ifelse(V0614 == 4, 0, 1), NA),
      
      ## Hearing
      disab_hear = ifelse(is.na(V0615), NA, ifelse(V0615 == 4, 0, 1)),
      disab_hear_f = ifelse(V0601 == 2, ifelse(V0615 == 4, 0, 1), NA),
      disab_hear_m = ifelse(V0601 == 1, ifelse(V0615 == 4, 0, 1), NA),
      
      ## Walking
      disab_mtr = ifelse(is.na(V0616), NA, ifelse(V0616 == 4, 0, 1)),
      disab_mtr_f = ifelse(V0601 == 2, ifelse(V0616 == 4, 0, 1), NA),
      disab_mtr_m = ifelse(V0601 == 1, ifelse(V0616 == 4, 0, 1), NA),
      
      ## Mental
      disab_mntl = ifelse(is.na(V0617), NA, ifelse(V0617 == 2, 0, 1)),
      disab_mntl_f = ifelse(V0601 == 2, ifelse(V0617 == 2, 0, 1), NA),
      disab_mntl_m = ifelse(V0601 == 1, ifelse(V0617 == 2, 0, 1), NA),
      
      ## Any
      disab = ifelse(disab_vis == 1 | disab_hear == 1 | disab_mtr == 1 | disab_mntl == 1, 1, 0),
      disab_f = ifelse(V0601 == 2, ifelse(disab_vis == 1 | disab_hear == 1 | disab_mtr == 1 | disab_mntl == 1, 1, 0), NA),
      disab_m = ifelse(V0601 == 1, ifelse(disab_vis == 1 | disab_hear == 1 | disab_mtr == 1 | disab_mntl == 1, 1, 0), NA),
      
      # Born in other municipality
      born_om = ifelse(is.na(V0618), NA, ifelse(V0618 == 1, 1, 0)),
      born_om_f = ifelse(V0601 == 2, ifelse(V0618 == 1, 1, 0), NA),
      born_om_m = ifelse(V0601 == 1, ifelse(V0618 == 1, 1, 0), NA),
      
      # Born in other uf
      born_os = ifelse(is.na(V0619), NA, ifelse(V0619 == 1, 1, 0)),
      born_os_f = ifelse(V0601 == 2, ifelse(V0619 == 1, 1, 0), NA),
      born_os_m = ifelse(V0601 == 1, ifelse(V0619 == 1, 1, 0), NA),
      
      # Literacy
      lit = ifelse(is.na(V0627), NA, ifelse(V0627 == 1, 1,0)),
      lit_f = ifelse(V0601 == 2, ifelse(V0627 == 1, 1, 0), NA),
      lit_m = ifelse(V0601 == 1, ifelse(V0627 == 1, 1, 0), NA),

      # School attendance
      `6to11as` = ifelse(is.na(V0628) | `6to11` != 1, NA, ifelse(V0628 %in% c(1,2), 1, 0)),
      `6to11as_f` = ifelse(V0601 == 2 & `6to11_f` == 1, ifelse(V0628 %in% c(1,2), 1, 0), NA),
      `6to11as_m` = ifelse(V0601 == 1 & `6to11_m` == 1, ifelse(V0628 %in% c(1,2), 1, 0), NA),
      
      `12to17as` = ifelse(is.na(V0628) | `12to17` != 1, NA, ifelse(V0628 %in% c(1,2), 1, 0)),
      `12to17as_f` = ifelse(V0601 == 2 & `12to17_f` == 1, ifelse(V0628 %in% c(1,2), 1, 0), NA),
      `12to17as_m` = ifelse(V0601 == 1 & `12to17_m` == 1, ifelse(V0628 %in% c(1,2), 1, 0), NA),
      
      `18to24as` = ifelse(is.na(V0628) | `18to24` != 1, NA, ifelse(V0628 %in% c(1,2), 1, 0)),
      `18to24as_f` = ifelse(V0601 == 2 & `18to24_f` == 1, ifelse(V0628 %in% c(1,2), 1, 0), NA),
      `18to24as_m` = ifelse(V0601 == 1 & `18to24_m` == 1, ifelse(V0628 %in% c(1,2), 1, 0), NA),
      
      # Employment
      emp = ifelse(is.na(V6910), NA, ifelse(V6910 == 1, 1, 0)),
      emp_f = ifelse(V0601 == 2, ifelse(V6910 == 1, 1, 0), NA),
      emp_m = ifelse(V0601 == 1, ifelse(V6910 == 1, 1, 0), NA),
      
      unem = ifelse(is.na(V6910), NA, ifelse(V6910 == 2, 1, 0)),
      unem_f = ifelse(V0601 == 2, ifelse(V6910 == 2, 1, 0), NA),
      unem_m = ifelse(V0601 == 1, ifelse(V6910 == 2, 1, 0), NA),
      
      econia = ifelse(is.na(V6900), NA, ifelse(V6900 == 2, 1, 0)),
      econia_f = ifelse(V0601 == 2, ifelse(V6900 == 2, 1, 0), NA),
      econia_m = ifelse(V0601 == 1, ifelse(V6900 == 2, 1, 0), NA),
      
      econa = ifelse(is.na(V6900), NA, ifelse(V6900 == 1, 1, 0)),
      econa_f = ifelse(V0601 == 2, ifelse(V6900 == 1, 1, 0), NA),
      econa_m = ifelse(V0601 == 1, ifelse(V6900 == 1, 1, 0), NA)
     ) %>% 
    select(
      # Weight
      weight,
      #Geoids
      uf_code, mun_code, wa_id,
      reg_code, mesoreg_code, microreg_code, 
      tot_m, tot_f,
      
      `0to5`, `0to5_f`, `0to5_m`,
      `6to11`, `6to11_f`, `6to11_m`,
      `12to17`, `12to17_f`, `12to17_m`,
      `18to24`, `18to24_f`, `18to24_m`,
      `25to64`, `25to64_f`, `25to64_m`,
      `65m`, `65m_f`, `65m_m`,

           tdr, chddr, olddr,
           wht, wht_f, wht_m,
           black, black_f, black_m,
           yellow, yellow_f, yellow_m,
           brown, brown_f, brown_m,
           indig, indig_f, indig_m,
           disab_vis, disab_vis_f, disab_vis_m,
           disab_hear, disab_hear_f, disab_hear_m,
           disab_mtr, disab_mtr_f, disab_mtr_m,
           disab_mntl, disab_mntl_f, disab_mntl_m,
           disab, disab_f, disab_m,
           born_om, born_om_f, born_om_m,
           born_os, born_os_f, born_os_m,
           lit, lit_f, lit_m,
           `6to11as`, `6to11as_f`, `6to11as_m`,
           `12to17as`, `12to17as_f`, `12to17as_m`,
           `18to24as`, `18to24as`, `18to24as_m`,
           emp, emp_f, emp_m,
           unem, unem_f, unem_m,
           econia, econia_f, econia_m,
           econa, econa_f, econa_m) 
    
    }
      
# File paths
input_folder <- "01_data/01_raw/dados_csv2010/pes"
dir.create("01_data/02_proc/01_recoded/pop")
dir.create("01_data/02_proc/01_recoded/pop")
output_folder <- "01_data/02_proc/01_recoded/pop"

# Get list of uf folders
uf_folders <- list.dirs(input_folder, recursive = FALSE)

# Process each uf folder
for (uf_folder in uf_folders) {
  uf_name <- basename(uf_folder)
  
  # Get list of municipality CSV files
  file_list <- list.files(uf_folder, pattern = "\\.csv$", full.names = TRUE)
  
  # Process each file
  for (file in file_list) {
    df <- fread(file, showProgress = FALSE)
    
    # Extract wa_id from the file name
    wa_id <- tools::file_path_sans_ext(basename(file))
    
    # Transform the data
    df_transformed <- transform_data(df)
    
    # Split the transformed data by wa_id
    wa_id_split <- split(df_transformed, df_transformed$wa_id)
    
    # Process each split by wa_id
    for (wa_id_name in names(wa_id_split)) {
      df_wa_id <- wa_id_split[[wa_id_name]]
      
      # Get unique reg_codes from the transformed data
      reg_codes <- unique(df_wa_id$reg_code)
      
      # Create a folder for each reg_code and save the corresponding files
      for (reg_code in reg_codes) {
        # Define the output directory for the reg_code
        reg_code_folder <- file.path(output_folder, as.character(reg_code))
        if (!dir.exists(reg_code_folder)) {
          dir.create(reg_code_folder, recursive = TRUE)
        }
        
        # Construct the new file name and output path
        new_file_name <- paste0(wa_id_name, ".csv")
        output_file <- file.path(reg_code_folder, new_file_name)
        
        # Save the transformed data to the new folder
        fwrite(df_wa_id, output_file)
      }
    }
  }
}
```
**Final Structure**
The processed population and dwelling files are saved in the `output_folder`, organized by region into folders, with individual files sorted by wa.

As previously noted, grouping this data by metropolitan area is not feasible. Therefore, before estimating new variables at each level, we will first download the necessary boundary data to obtain the required geocodes.

# III. Boundaries `bra_00_boundaries.R`
To get the boundaries corresponding to the Brazilian Census 2010,we
heavily relies on the **geobr** library to process geographic boundaries and demographic data for various administrative levels in Brazil from the 2010 Census. The geobr package is utilized to read geospatial data for all geographical levels considered in the census. Each dataset retrieved using **geobr** is renamed and organized into a consistent format before being saved as GeoJSON files for further use. Additionally, given the substantial size of the census tract data, we also split these files by region to facilitate more manageable processing.

```{r}
# set up ------------------------------------------------------------------
# Clear environment
rm(list = ls())

# Load necessary libraries
library(dplyr)
library(geobr)
library(sf)
library(lwgeom)
library(readxl)

ids <- read_excel("00_doc/Documentacao/Documentação/Divisão Territorial do Brasil/Unidades da Federação, Mesorregiões, microrregiões e municípios 2010_modif.xls")

setwd("01_data/01_raw/boundaries/")

# downloading -------------------------------------------------------------
nat <- read_country(year=2010) |> 
    st_write("nat.geojson")

reg <- read_region(year = 2010) |> 
  setNames(c("reg_code", "reg_name", "geom")) |> 
  st_write("reg.geojson", delete_dsn = TRUE)

uf <- read_state(year=2010) |> 
  setNames(c("uf_code", "uf_name", "reg_code", "reg_name", "geom")) |> 
  st_write("uf.geojson")

uf_codes <- unique(uf$abbrev_state) # get a vector of uf codes to download census tract files

uf <- read_state(year=2010) |> 
  select(-abbrev_state)  
  
mesoreg <- read_meso_region(year=2010) |> 
  select(-abbrev_state) |> 
  setNames(c("uf_code", "uf_name", "mesoreg_id", "mesoreg_name", "geom")) |> 
  st_write("mesoreg.geojson")

microreg <- read_micro_region(year=2010) |> 
  select(-abbrev_state) |> 
  setNames(c("uf_code", "uf_name", "microreg_id", "microreg_name", "geom")) |> 
  st_write("microreg.geojson")

metro <- read_metro_area(year=2010) |> 
  select(-abbrev_state) |> 
  setNames(c("mun_id", "mun_name", "uf_code", "metro_name", "metro_type", "metro_legislation_date", "geom"))|> 
  st_write("metro.geojson")

mun_10 <- read_municipality(year=2010)
mun_20 <- read_municipality(year=2020)
mun <- read_municipality(year=2010) |> 
  select(-abbrev_state) |> 
  setNames(c("mun_id", "mun_name", "uf_code", "geom")) |> 
  st_write("mun.geojson")

wa <- read_weighting_area(year=2010) |> 
  select(-abbrev_state) |> 
  setNames(c("wa_id","mun_id", "mun_name", "uf_code", "reg_code", "reg_name","geom")) |> 
  st_write("wa.geojson")

# geo_levels_names --------------------------------------------------------
# Define a function to process each data frame
process_df <- function(df, select_cols = NULL) {
  df <- df %>%
    st_drop_geometry() %>%
    as.data.frame() %>%
    mutate(across(everything(), as.character))
  
  if (!is.null(select_cols)) {
    df <- df %>% select(all_of(select_cols))
  }
  
  return(df)
}

# Process each data frame
mun_names <- process_df(mun)
uf_names <- process_df(uf)
reg_names <- process_df(reg)
mesoreg_names <- process_df(mesoreg, c("mesoreg_id", "mesoreg_name"))
microreg_names <- process_df(microreg, c("microreg_id", "microreg_name"))
metro_names <- process_df(metro) %>% select(-mun_name)

# Join data frames
names0 <- full_join(ids, mun_names, by = c("uf_code", "mun_id"))
names <- full_join(uf_names, reg_names) %>%
  full_join(names0) %>%
  full_join(mesoreg_names) %>%
  full_join(microreg_names) %>%
  full_join(metro_names)

# Write to CSV
write.csv(names, "geo_levels_names_full.csv", row.names = FALSE, fileEncoding = "latin1")
# census tract ------------------------------------------------------------

# Create directories for census tract data
dir.create("tract_uf/", showWarnings = FALSE)

# Function to process and save census tract data for each state
process_census_tract <- function(state_code) {
  current_state_data <- read_census_tract(
    code_tract = state_code, 
    year = 2010,
    showProgress = FALSE
  ) %>%
    mutate(uf_code = substr(code_subdistrict, 1, 2)) %>%
    select(code_state, code_muni, name_muni, code_tract) %>%
    rename(uf_code = code_state,
           tract_id = code_tract,
           mun_id = code_muni,
           mun_name = name_muni)
  
  st_write(current_state_data, 
           paste0("01_data/01_raw/boundaries/tract_uf/", state_code, ".geojson"), 
           delete_dsn = TRUE)
}

# Process census tract data for each state
lapply(uf_codes, process_census_tract)

# Combine all state GeoJSON files into one
folder_path <- "tract_uf"
geojson_files <- list.files(path = folder_path, pattern = "\\.geojson$", 
                            full.names = TRUE)
tract <- geojson_files %>%
  lapply(st_read) %>%
  bind_rows() %>%
  select(-uf_code, -mun_name)

st_write(tract, "tract.geojson", delete_dsn = TRUE)
```

# IV. Variables and indicators `bra_02_indicators`
Now, with dwellings and population data we are going to create new variables deriving both absolute counts and relative proportions for each geographical level.

In short, the code processes large-scale dwelling data using survey functions from the survey and *srvyr* packages in R to accurately estimate and summarize key dwelling and population characteristics. It begins by specifying input and output directories and defining variables of interest (*dwg_vars* and *pop_vars*). The data is then converted into a survey design object using `as_survey_design`, which accounts for the complex sampling design and ensures that all calculations reflect the survey's representativeness. The functions `survey_total` and `survey_mean` are used to compute totals and means for various dwelling attributes, along with their coefficients of variation (cv) to assess estimate precision. The data is grouped by specified variables, and the results are processed in parallel across multiple cores to enhance efficiency. After processing, the results are combined, and unreliable estimates (those with a cv greater than 0.3) are filtered out.

**Set up**
```{r}
# Clear all objects from the workspace
rm(list = ls())

# Set global options to avoid scientific notation for numbers
options(scipen = 999)

# Load necessary libraries
library(tidyverse)   # Collection of R packages for data science (ggplot2, dplyr, etc.)
library(data.table)  # For fast data manipulation and aggregation
library(dplyr)       # Part of tidyverse, for data manipulation
library(readr)       # For reading and writing data
library(tidyr)       # For tidying data, part of tidyverse
library(fs)          # For interacting with the file system
library(purrr)       # For functional programming, part of tidyverse
library(survey)      # For analyzing survey data
library(srvyr)       # Tidyverse-style survey package
library(rlang)       # Tools to work with core language features of R
library(parallel)    # For parallel processing
library(foreach)     # For looping constructs in parallel processing
library(doParallel)  # To register a parallel backend with foreach

# Detect the number of available CPU cores and leave one free for system processes
num_cores <- detectCores() - 1

# Create a cluster using the detected number of cores for parallel processing
cl <- makeCluster(num_cores)

# Register the cluster for parallel processing with foreach
registerDoParallel(cl)
```

## Dwellings

**Functions**
```{r}
# Define the input folder containing the raw data
input_folder <- "01_data/02_proc/01_recoded/dwg"

# Create the output directories 
dir.create("01_data/02_proc/02_indicators", showWarnings = FALSE)
dir.create("01_data/02_proc/02_indicators/dwg", showWarnings = FALSE)

# Define the output folder where processed files will be saved
output_folder <- "01_data/02_proc/02_indicators/dwg"

# Define the list of dwelling variables to be used in the summary functions
dwg_vars <- c("urban", "rural",
              "owned", "rented", "loaned", "other",
              "walls_masonry", "walls_wood", "walls_rearth", "walls_straw", "walls_other", "nwalls",
              "1rm", "2rms", "3mrms", "1bedrm", "2mbedrms",
              "novc", "bthrm", "toilet", "obdhnb",
              "ss_sewage", "ss_pit", "ss_ditch", "ss_river", "ss_other",
              "water_netwk", "water_well", "water_truck", "water_rain", "water_river", "water_other",
              "garbage_clngsvcs", "garbage_burnt", "garbage_buried", "garbage_throwed", "garbage_other",
              "elect", "inet", "tv", "wm", "refri", "cp", "cmp", "mtrcl", "car")

# Define a custom summary function to calculate totals and means for the specified variables
dwg_summ <- function(data, group_var) {
  data %>%
    as_survey_design(weights = weight) %>%  # Apply survey design using the weight variable
    group_by(across(all_of(c(group_var)))) %>%  # Group data by the specified grouping variable
    summarize(
      # Calculate total for each variable, and create new column names prefixed with "d_"
      across(all_of(dwg_vars), 
             ~ survey_total(., vartype = "cv", na.rm = TRUE), 
             .names = "d_{.col}"),
      d_dwg = survey_total(vartype = "cv", na.rm = TRUE),  # Calculate total dwellings
      # Calculate proportions for each variable, and create new column names prefixed with "d_prop_"
      across(all_of(dwg_vars), 
             ~ survey_mean(., vartype = "cv", na.rm = TRUE), 
             .names = "d_prop_{.col}"),
      # Calculate various mean values
      d_mean_bedrm_occup = survey_mean(rm_occup, vartype = "cv", na.rm = TRUE),
      d_mean_income = survey_mean(income, vartype = "cv", na.rm = TRUE),
      d_mean_pcincome = survey_mean(pcincome, vartype = "cv", na.rm = TRUE),
      d_mean_rm_occup = survey_mean(rm_occup, vartype = "cv", na.rm = TRUE),
      d_mean_rv = survey_mean(rv, vartype = "cv", na.rm = TRUE)
    )
}

# Define an extended summary function that groups by additional geographic levels
dwg_summ_2 <- function(data, geo_levels, group_var) {
  data %>%
    as_survey_design(weights = weight) %>%  # Apply survey design using the weight variable
    group_by(across(all_of(c(geo_levels, group_var)))) %>%  # Group data by geographic levels and specified group variable
    summarize(
      # Calculate total for each variable, and create new column names prefixed with "d_"
      across(all_of(dwg_vars), 
             ~ survey_total(., vartype = "cv", na.rm = TRUE), 
             .names = "d_{.col}"),
      d_dwg = survey_total(vartype = "cv", na.rm = TRUE),  # Calculate total dwellings
      # Calculate proportions for each variable, and create new column names prefixed with "d_prop_"
      across(all_of(dwg_vars), 
             ~ survey_mean(., vartype = "cv", na.rm = TRUE), 
             .names = "d_prop_{.col}"),
      # Calculate various mean values
      d_mean_bedrm_occup = survey_mean(rm_occup, vartype = "cv", na.rm = TRUE),
      d_mean_income = survey_mean(income, vartype = "cv", na.rm = TRUE),
      d_mean_pcincome = survey_mean(pcincome, vartype = "cv", na.rm = TRUE),
      d_mean_rm_occup = survey_mean(rm_occup, vartype = "cv", na.rm = TRUE),
      d_mean_rv = survey_mean(rv, vartype = "cv", na.rm = TRUE)
    )
}

# Define the main directory containing the regional folders
main_directory <- "01_data/02_proc/01_recoded/dwg"

# List all subdirectories within the main directory, which correspond to different regions
reg_folders <- list.dirs(main_directory, recursive = FALSE)

# Export necessary custom functions and variables to the cluster for parallel processing
clusterExport(cl, varlist = c("dwg_summ", "dwg_summ_2", "as_survey_design", "survey_total", "survey_mean"))

# Load required libraries on each node in the cluster
clusterEvalQ(cl, {
  library(data.table)
  library(dplyr)
})

# Define a function to process dwelling data
process_dwg <- function(results_list) {
  # Combine all data frames in the results list into a single data frame
  dwg_ind <- bind_rows(results_list)
  
  # Identify columns that need modification based on the condition (CV > 0.3)
  columns_to_modify <- names(dwg_ind)[grepl("^d_", names(dwg_ind)) & !grepl("_cv$", names(dwg_ind))]

  # Apply the condition to set values to NA where CV is greater than 0.3, and remove _cv columns
  dwg_ind <- dwg_ind %>%
    mutate(across(all_of(columns_to_modify), ~ ifelse(get(paste0(cur_column(), "_cv")) > 0.3, NA, .))) %>%
    select(-ends_with("_cv"))
  
  # Return the modified data frame
  return(dwg_ind)
}
```

These functions will be used to estimate indicators at various geographical levels. It is important to note that for metropolitan areas, the data needed to be merged with a mapping file containing the necessary IDs before proceeding with the estimation

The following code performs data processing for all levels except census tract using parallel computing to speed up execution. At first, it initializes empty lists to store results and then processes data for each folder corresponding to each level. For each folder, it lists and combines all CSV files into a single data frame, applies the appropriate summarization function (`dwg_summ` for regional and `dwg_summ_2` for the others), and stores the results. After processing, the code saves the final indicators for all levels to separate CSV files, ensuring that the processed data is organized and readily available for further analysis.
```{r}
# Region
# Initialize an empty list to store results from each folder
results_list <- list()

# Perform a parallel loop over each folder corresponding to reg_code
results_list <- foreach(reg_folder = reg_folders, .packages = c("data.table", "dplyr")) %dopar% {
  
  # List all CSV files in the current reg_code folder
  csv_files <- list.files(reg_folder, pattern = "*.csv", full.names = TRUE)
  
  # Read and combine all CSV files from the current folder into a single data frame
  combined_data <- bind_rows(lapply(csv_files, fread))
  
  # Apply the dwg_summ function to the combined data, grouped by reg_code
  result <- dwg_summ(combined_data, group_var = "reg_code")
  
  # Return the result for the current reg_code folder
  return(result)
}

# Process the list of results to obtain final indicators
dwg_ind <- process_dwg(results_list)

# Save the final indicators to a CSV file
fwrite(dwg_ind, "01_data/02_proc/02_indicators/dwg/dwg_reg.csv", row.names = FALSE)

# Mesoregion
# Initialize an empty list to store results
results_list <- list()

# Loop over each reg_code folder
results_list <- foreach(reg_folder = reg_folders, .packages = c("data.table", "dplyr")) %dopar% {
  # List all CSV files in the current reg_code folder
  csv_files <- list.files(reg_folder, pattern = "*.csv", full.names = TRUE)
  
  # Read and combine all CSV files in the current folder
  combined_data <- bind_rows(lapply(csv_files, fread))
  
  # Apply the dwg_summ_2 function to the combined data
  result <- dwg_summ_2(combined_data, geo_levels = c("reg_code", "uf_code"), group_var = "mesoreg_code")
  
  # Return the result for the current folder
  return(result)
}

dwg_ind <- process_dwg(results_list)

# Save the final result to a CSV file
fwrite(dwg_ind, "01_data/02_proc/02_indicators/dwg/dwg_mesoreg.csv", row.names = FALSE)

# Microregion
# Initialize an empty list to store results
results_list <- list()

# Loop over each reg_code folder
results_list <- foreach(reg_folder = reg_folders, .packages = c("data.table", "dplyr")) %dopar% {
  # List all CSV files in the current reg_code folder
  csv_files <- list.files(reg_folder, pattern = "*.csv", full.names = TRUE)
  
  # Read and combine all CSV files in the current folder
  combined_data <- bind_rows(lapply(csv_files, fread))
  
  # Apply the dwg_summ_2 function to the combined data
  result <- dwg_summ_2(combined_data, geo_levels = c("reg_code", "uf_code", "mesoreg_code"), group_var = "microreg_code")
  
  # Return the result for the current folder
  return(result)
}

dwg_ind <- process_dwg(results_list)

# Save the final result to a CSV file
fwrite(dwg_ind, "01_data/02_proc/02_indicators/dwg/dwg_microreg.csv", row.names = FALSE)

# Metropolitan area
mapping <- fread("01_data/01_raw/boundaries/geo_levels_names.csv", encoding="Latin-1")
mapping$mun_code <- substr(mapping$mun_id, 3,7)
mapping$mun_code <- as.integer(mapping$mun_code)

# Parallel loop over each reg_code folder
results_list <- foreach(reg_folder = reg_folders, .packages = c("data.table", "dplyr")) %dopar% {
  # List all CSV files in the current reg_code folder
  csv_files <- list.files(reg_folder, pattern = "*.csv", full.names = TRUE)
  
  # Read and combine all CSV files in the current folder
  combined_data <- bind_rows(lapply(csv_files, fread))
  
  # Merge with the mapping file to add mun_id and metro_name
  combined_data <- left_join(combined_data, mapping, by = c("uf_code", "mun_code"))
  
  # Apply the dwg_summ_2 function to the combined data
  result <- dwg_summ_2(combined_data, geo_levels = c("metro_name", "mun_id"), group_var = "metro_name")
  
  # Return the result for the current folder
  return(result)
}
dwg_ind <- process_dwg(results_list)
fwrite(dwg_ind, "01_data/02_proc/02_indicators/dwg/dwg_metro.csv", row.names = FALSE)

# Municipality
# Initialize an empty list to store results
results_list <- list()

# Loop over each reg_code folder
results_list <- foreach(reg_folder = reg_folders, .packages = c("data.table", "dplyr")) %dopar% {
  # List all CSV files in the current reg_code folder
  csv_files <- list.files(reg_folder, pattern = "*.csv", full.names = TRUE)
  
  # Read and combine all CSV files in the current folder
  combined_data <- bind_rows(lapply(csv_files, fread))
  
  # Apply the dwg_summ_2 function to the combined data
  result <- dwg_summ_2(combined_data, geo_levels = c("reg_code", "uf_code", "mesoreg_code", "microreg_code"), group_var = "mun_code")
  
  # Return the result for the current folder
  return(result)
}

dwg_ind <- process_dwg(results_list)

# Save the final result to a CSV file
fwrite(dwg_ind, "01_data/02_proc/02_indicators/dwg/dwg_mun.csv", row.names = FALSE)

# Weigthing Area
# Initialize an empty list to store results
results_list <- list()

# Loop over each reg_code folder
results_list <- foreach(reg_folder = reg_folders, .packages = c("data.table", "dplyr")) %dopar% {
  # List all CSV files in the current reg_code folder
  csv_files <- list.files(reg_folder, pattern = "*.csv", full.names = TRUE)
  
  # Read and combine all CSV files in the current folder
  combined_data <- bind_rows(lapply(csv_files, fread))
  
  # Apply the dwg_summ_2 function to the combined data
  result <- dwg_summ_2(combined_data, geo_levels = c("reg_code", "uf_code", "mesoreg_code", "microreg_code"), group_var = "wa_id")
  
  # Return the result for the current folder
  return(result)
}

dwg_ind <- process_dwg(results_list)

# Save the final result to a CSV file
fwrite(dwg_ind, "01_data/02_proc/02_indicators/dwg/dwg_wa.csv", row.names = FALSE)
```
A similar procedure is applied to the population data:
```{r}
dir.create("01_data/02_proc/02_indicators/pop", showWarnings = FALSE, recursive = TRUE)

pop_vars <- c("tot_m", "tot_f",
              "0to5", "0to5_f", "0to5_m",
              "6to11", "6to11_f", "6to11_m",
              "12to17", "12to17_f", "12to17_m",
              "18to24", "18to24_f", "18to24_m",
              "25to64", "25to64_f", "25to64_m",
              "65m", "65m_f", "65m_m",
              "tdr", "chddr", "olddr",
              "wht", "wht_f", "wht_m",
              "black", "black_f", "black_m",
              "yellow", "yellow_f", "yellow_m",
              "brown", "brown_f", "brown_m",
              "indig", "indig_f", "indig_m",
              "disab_vis", "disab_vis_f", "disab_vis_m",
              "disab_hear", "disab_hear_f", "disab_hear_m",
              "disab_mtr", "disab_mtr_f", "disab_mtr_m",
              "disab_mntl", "disab_mntl_f", "disab_mntl_m",
              "disab", "disab_f", "disab_m",
              "born_om", "born_om_f", "born_om_m",
              "born_os", "born_os_f", "born_os_m",
              "lit", "lit_f", "lit_m",
              "6to11as", "6to11as_f", "6to11as_m",
              "12to17as", "12to17as_f", "12to17as_m",
              "18to24as", "18to24as", "18to24as_m",
              "emp", "emp_f", "emp_m",
              "unem", "unem_f", "unem_m",
              "econia", "econia_f", "econia_m",
              "econa", "econa_f", "econa_m"
)

input_folder <- "01_data/02_proc/01_recoded/pop"

pop_summ <- function(data) {
  data %>%
    as_survey_design(weights = weight) %>% 
    group_by(across(all_of(c(group_var)))) %>%
    summarize(
      across(all_of(pop_vars), 
             ~ survey_total(., vartype = "cv", na.rm = TRUE), 
             .names = "p_{.col}"),
      d_pop = survey_total(vartype = "cv", na.rm = TRUE),
      across(all_of(pop_vars), 
             ~ survey_mean(., vartype = "cv", na.rm = TRUE), 
             .names = "p_prop_{.col}")
    )
}

pop_summ_2 <- function(data) {
  data %>%
    as_survey_design(weights = weight) %>% 
    group_by(across(all_of(c(geo_levels, group_var)))) %>%
    summarize(
      across(all_of(pop_vars), 
             ~ survey_total(., vartype = "cv", na.rm = TRUE), 
             .names = "p_{.col}"),
      d_pop = survey_total(vartype = "cv", na.rm = TRUE),
      across(all_of(pop_vars), 
             ~ survey_mean(., vartype = "cv", na.rm = TRUE), 
             .names = "p_prop_{.col}")
    )
}

# Define the directory containing the folders
main_directory <- "01_data/02_proc/01_recoded/pop"

# List all uf_code folders
reg_folders <- list.dirs(main_directory, recursive = FALSE)

# Export necessary custom functions and variables to the cluster
clusterExport(cl, varlist = c("pop_summ", "pop_summ_2", "as_survey_design", "survey_total", "survey_mean"))  # Add other custom functions if needed

# Load required libraries on the cluster nodes
clusterEvalQ(cl, {
  library(data.table)
  library(dplyr)
})

# Define the function
process_pop <- function(results_list) {
  # Bind rows from the results list
  pop_ind <- bind_rows(results_list)
  
  # Identify columns to modify
  columns_to_modify <- names(pop_ind)[grepl("^d_", names(pop_ind )) & !grepl("_cv$", names(pop_ind ))]
  
  # Apply the condition and remove _cv columns
  pop_ind  <- pop_ind  %>%
    mutate(across(all_of(columns_to_modify), ~ ifelse(get(paste0(cur_column(), "_cv")) > 0.3, NA, .))) %>%
    select(-ends_with("_cv"))
  return(pop_ind )
}


## reg ----------------------------------------------------------------
results_list <- list() # Initialize the results list

# Parallel loop over each reg_code folder
results_list <- foreach(reg_folder = reg_folders, .packages = c("data.table", "dplyr")) %dopar% {
  # List all CSV files in the current reg_code folder
  csv_files <- list.files(reg_folder, pattern = "*.csv", full.names = TRUE)
  
  # Read and combine all CSV files in the current folder
  combined_data <- bind_rows(lapply(csv_files, fread))
  
  # Apply the pop_summ_wa function to the combined data
  result <- pop_summ(combined_data, group_var = "reg_code")
  
  # Return the result for the current folder
  return(result)
}

pop_ind <- process_pop(results_list)

# Save the final result to a CSV file
fwrite(pop_ind , "01_data/02_proc/02_indicators/pop/pop_reg.csv", row.names = FALSE)

## uf -------------------------------------------------------------------
# Initialize an empty list to store results
results_list <- list()

# Loop over each reg_code folder
results_list <- foreach(reg_folder = reg_folders, .packages = c("data.table", "dplyr")) %dopar% {
  # List all CSV files in the current reg_code folder
  csv_files <- list.files(reg_folder, pattern = "*.csv", full.names = TRUE)
  
  # Read and combine all CSV files in the current folder
  combined_data <- bind_rows(lapply(csv_files, fread))
  
  # Apply the pop_summ_2 function to the combined data
  result <- pop_summ_2(combined_data, geo_levels = c("reg_code"), group_var = "uf_code")
  
  # Return the result for the current folder
  return(result)
}

pop_ind <- process_pop(results_list)

# Save the final result to a CSV file
fwrite(pop_ind, "01_data/02_proc/02_indicators/pop/pop_uf.csv", row.names = FALSE)

## mesoreg -----------------------------------------------------------------
# Initialize an empty list to store results
results_list <- list()

# Loop over each reg_code folder
results_list <- foreach(reg_folder = reg_folders, .packages = c("data.table", "dplyr")) %dopar% {
  # List all CSV files in the current reg_code folder
  csv_files <- list.files(reg_folder, pattern = "*.csv", full.names = TRUE)
  
  # Read and combine all CSV files in the current folder
  combined_data <- bind_rows(lapply(csv_files, fread))
  
  # Apply the pop_summ_2 function to the combined data
  result <- pop_summ_2(combined_data, geo_levels = c("reg_code", "uf_code"), group_var = "mesoreg_code")
  
  # Return the result for the current folder
  return(result)
}

pop_ind <- process_pop(results_list)

# Save the final result to a CSV file
fwrite(pop_ind, "01_data/02_proc/02_indicators/pop/pop_mesoreg.csv", row.names = FALSE)

## microreg -----------------------------------------------------------------
# Initialize an empty list to store results
results_list <- list()

# Loop over each reg_code folder
results_list <- foreach(reg_folder = reg_folders, .packages = c("data.table", "dplyr")) %dopar% {
  # List all CSV files in the current reg_code folder
  csv_files <- list.files(reg_folder, pattern = "*.csv", full.names = TRUE)
  
  # Read and combine all CSV files in the current folder
  combined_data <- bind_rows(lapply(csv_files, fread))
  
  # Apply the pop_summ_2 function to the combined data
  result <- pop_summ_2(combined_data, geo_levels = c("reg_code", "uf_code", "mesoreg_code"), group_var = "microreg_code")
  
  # Return the result for the current folder
  return(result)
}

pop_ind <- process_pop(results_list)

# Save the final result to a CSV file
fwrite(pop_ind, "01_data/02_proc/02_indicators/pop/pop_microreg.csv", row.names = FALSE)

## metro -------------------------------------------------------------------
# Parallel loop over each reg_code folder
results_list <- foreach(reg_folder = reg_folders, .packages = c("data.table", "dplyr")) %dopar% {
  # List all CSV files in the current reg_code folder
  csv_files <- list.files(reg_folder, pattern = "*.csv", full.names = TRUE)
  
  # Read and combine all CSV files in the current folder
  combined_data <- bind_rows(lapply(csv_files, fread))
  
  # Merge with the mapping file to add mun_id and metro_name
  combined_data <- left_join(combined_data, mapping, by = c("uf_code", "mun_code"))
  
  # Apply the pop_summ_2 function to the combined data
  result <- pop_summ_2(combined_data, geo_levels = c("metro_name", "mun_id"), group_var = "metro_name")
  
  # Return the result for the current folder
  return(result)
}
pop_ind <- process_pop(results_list)
fwrite(pop_ind, "01_data/02_proc/02_indicators/pop/pop_metro.csv", row.names = FALSE)

## mun -----------------------------------------------------------------
# Initialize an empty list to store results
results_list <- list()

# Loop over each reg_code folder
results_list <- foreach(reg_folder = reg_folders, .packages = c("data.table", "dplyr")) %dopar% {
  # List all CSV files in the current reg_code folder
  csv_files <- list.files(reg_folder, pattern = "*.csv", full.names = TRUE)
  
  # Read and combine all CSV files in the current folder
  combined_data <- bind_rows(lapply(csv_files, fread))
  
  # Apply the pop_summ_2 function to the combined data
  result <- pop_summ_2(combined_data, geo_levels = c("reg_code", "uf_code", "mesoreg_code", "microreg_code"), group_var = "mun_code")
  
  # Return the result for the current folder
  return(result)
}

pop_ind <- process_pop(results_list)

# Save the final result to a CSV file
fwrite(pop_ind, "01_data/02_proc/02_indicators/pop/pop_mun.csv", row.names = FALSE)

## wa -----------------------------------------------------------------
# Initialize an empty list to store results
results_list <- list()

# Loop over each reg_code folder
results_list <- foreach(reg_folder = reg_folders, .packages = c("data.table", "dplyr")) %dopar% {
  # List all CSV files in the current reg_code folder
  csv_files <- list.files(reg_folder, pattern = "*.csv", full.names = TRUE)
  
  # Read and combine all CSV files in the current folder
  combined_data <- bind_rows(lapply(csv_files, fread))
  
  # Apply the pop_summ_2 function to the combined data
  result <- pop_summ_2(combined_data, geo_levels = c("reg_code", "uf_code", "mesoreg_code", "microreg_code", "mun_code"), group_var = "wa_id")
  
  # Return the result for the current folder
  return(result)
}

pop_ind <- process_pop(results_list)

# Save the final result to a CSV file
fwrite(pop_ind, "01_data/02_proc/02_indicators/pop/pop_wa.csv", row.names = FALSE)

```

## Census tract `bra_01_recoding-ct.R`
Finally, at the census tract level, we didn't need to perform the previous step of recoding because the predefined information had already been estimated. It's important to note that this  database contains some missing values due to privacy concerns, as this level of aggregation is very detailed.

The following code processes dwellingd (*domicilio01*) and population (*pessoa03*) data by reading multiple Excel files from the  specified directories, combining them into single data frames, and calculating a variety of summary statistics. Finally, both datasets are processed to include only specific columns of interest. 
```{r}
# set up 
rm(list=ls())

library(data.table)
library(dplyr)
library(readxl)

# domicilio01 

# Set the directory path
directory_path <- "01_data/01_raw/sc/01_xls_topic/Domicilio01"

# Get a list of XLS files in the directory
file_list <- list.files(directory_path, pattern = "\\.xls$", full.names = TRUE)

# Initialize an empty list to store data frames
data_list <- list()

# Loop through the list of files and read them into data frames
for (file in file_list) {
  data <- read_excel(file)
  data_list[[file]] <- data
}

# Combine all data frames into one
domicilio01 <- do.call(rbind, data_list)

domicilio01 <- domicilio01 %>%
  mutate_at(vars(-Cod_setor, -Situacao_setor), as.numeric)

domicilio01 <- domicilio01 %>% mutate(
                                      tract_id = Cod_setor,
                                      d_tot=V002,
                                      d_own = V006 + V007,
                                      d_prop_own = (V006 + V007) / V002 * 100,
                                      d_rented=V008,
                                      d_prop_rented=V008/V002*100,
                                      d_loaned=V009 + V010,
                                      d_prop_loaned=(V009 + V010)/V002*100,
                                      d_other=V011,
                                      d_prop_other=V011/V002*100,
                                      d_water_network=V012,
                                      d_prop_water_network=V012/V002*100,
                                      d_water_well_in=V013,
                                      d_prop_water_well_in=V013/V002*100,
                                      d_water_rain=V014,
                                      d_prop_water_rain=V014/V002*100,
                                      d_water_other=V015,
                                      d_prop_water_other=V015/V002*100,
                                      d_toil=V016,
                                      d_prop_toil=V016/V002*100,
                                      d_ss_network=V017,
                                      d_prop_ss_network=V017/V002*100,
                                      d_ss_pit=V018+V019,
                                      d_prop_ss_pit=(V018+V019)/V002*100,
                                      d_ss_ditch=V020,
                                      d_prop_ss_ditch=V020/V002*100,
                                      d_ss_river=V021,
                                      d_prop_ss_river=V021/V002*100,
                                      d_ss_other=V022,
                                      d_prop_ss_other=V022/V002*100,
                                      d_ntoil=V023,
                                      d_prop_ntoil=V023/V002*100,
                                      d_toil=V024,
                                      d_prop_toil=V024/V002*100,
                                      d_ntoil=V034,
                                      d_prop_ntoil=V034/V002*100,
                                      d_garbage=V035,
                                      d_prop_garbage=V035/V002*100,
                                      d_garbage_clngsvcs1=V036,
                                      d_prop_garbage_clngsvcs1=V036/V002*100,
                                      d_garbage_clngsvcs2=V037,
                                      d_prop_garbage_clngsvcs2=V037/V002*100,
                                      d_garbage_burnt=V038,
                                      d_prop_garbage_burnt=V038/V002*100,
                                      d_garbage_buried=V039,
                                      d_prop_garbage_buried=V039/V002*100,
                                      d_garbage_throwed1=V040,
                                      d_prop_garbage_throwed1=V040/V002*100,
                                      d_garbage_throwed2=V041,
                                      d_prop_garbage_throwed2=V041/V002*100,
                                      d_garbage_other=V042,
                                      d_prop_garbage_other=V042/V002*100,
                                      d_elect=V043,
                                      d_prop_elect=V043/V002*100,
                                      d_elect1=V044,
                                      d_elect2=V045,
                                      d_nelect=V046,
                                      d_1occup=V050,
                                      d_2occup=V051,
                                      d_3occup=V052,
                                      d_4occup=V053,
                                      d_5occup=V054,
                                      d_6occup=V055,
                                      d_7occup=V056,
                                      d_8occup=V057,
                                      d_9occup=V058,
                                      d_10moccup=V059,
                                      d_mh_2occup=V062,
                                      d_mh_3occup=V063,
                                      d_mh_4occup=V064,
                                      d_mh_5occup=V065,
                                      d_mh_6occup=V066,
                                      d_mh_7moccup=V067,
                                      d_mh_1occup=V068,
                                      d_fh_2occup=V081,
                                      d_fh_3occup=V082,
                                      d_fh_4occup=V083,
                                      d_fh_5occup=V084,
                                      d_fh_6occup=V085,
                                      d_fh_7moccup=V086,
                                      d_fh_1occup=V087)
domicilio01 <- select(domicilio01,
                      244:319)
# pessoa03 ----------------------------------------------------------------
# Set the directory path
directory_path <- "01_data/01_raw/sc/01_xls_topic/Pessoa03"

# Get a list of XLS files in the directory
file_list <- list.files(directory_path, pattern = "\\.xls$", full.names = TRUE)

# Initialize an empty list to store data frames
data_list <- list()

# Loop through the list of files and read them into data frames
for (file in file_list) {
  data <- read_excel(file)
  data_list[[file]] <- data
}

# Combine all data frames into one
pessoa03 <- do.call(rbind, data_list)
pessoa03 <- pessoa03 %>%
  mutate_at(vars(-Cod_setor, -Situacao_setor), as.numeric)

pessoa03 <- pessoa03 %>%  mutate(
  tract_id = Cod_setor,
  p_tot=V001,
  p_wht=V002,
  p_prop_wht=V002/V001 * 100,
  p_black=V003,
  p_prop_black=V003/V001 * 100,
  p_yellow=V004,
  p_prop_yellow=V004/V001 * 100,
  p_brown=V005,
  p_prop_brown=V005/V001 * 100,
  p_indig=V006,
  p_prop_indig=V006/V001 * 100,
  p_0to4wh=V007,
  p_0to4bl=V008,
  p_0to4ye=V009,
  p_0to4br=V010,
  p_0to4in=V011,
  p_0to4=V007 + V008 + V009 + V010 + V011,
  p_5to9wh=V012,
  p_5to9bl=V013,
  p_5to9ye=V014,
  p_5to9br=V015,
  p_5to9in=V016,
  p_5to9=V012 + V013 + V014 + V015 + V016,
  p_10to14wh=V017,
  p_10to14bl=V018,
  p_10to14ye=V019,
  p_10to14br=V020,
  p_10to14in=V021,
  p_10to14=V017 + V018 + V019 + V020 + V021,
  p_15to19wh=V022,
  p_15to19bl=V023,
  p_15to19ye=V024,
  p_15to19br=V025,
  p_15to19in=V026,
  p_15to19=V022 + V023 + V024 + V025 +V026,
  p_15to17wh=V027,
  p_15to17bl=V028,
  p_15to17ye=V029,
  p_15to17br=V030,
  p_15to17in=V031,
  p_15to17=V027 + V028 + V029 + V030 + V031,
  p_18or19wh=V032,
  p_18or19bl=V033,
  p_18or19ye=V034,
  p_18or19br=V035,
  p_18or19in=V036,
  p_18or19=V032 + V033 + V034 + V035 + V036,
  p_20to24wh=V037,
  p_20to24bl=V038,
  p_20to24ye=V039,
  p_20to24br=V040,
  p_20to24in=V041,
  p_20to24=V037 + V038 + V039 + V040 + V041,
  p_25to29wh=V042,
  p_25to29bl=V043,
  p_25to29ye=V044,
  p_25to29br=V045,
  p_25to29in=V046,
  p_25to29=V042 + V043 + V044 + V045 + V046,
  p_30to34wh=V047,
  p_30to34bl=V048,
  p_30to34ye=V049,
  p_30to34br=V050,
  p_30to34in=V051,
  p_30to34=V047 + V048 + V049 + V050 + V051,
  p_35to39wh=V052,
  p_35to39bl=V053,
  p_35to39ye=V054,
  p_35to39br=V055,
  p_35to39in=V056,
  p_35to39=V052 + V053 + V054 + V055 + V056,
  p_40to44wh=V057,
  p_40to44bl=V058,
  p_40to44ye=V059,
  p_40to44br=V060,
  p_40to44in=V061,
  p_40to44=V057 + V058 + V059 + V060 + V061,
  p_45to49wh=V062,
  p_45to49bl=V063,
  p_45to49ye=V064,
  p_45to49br=V065,
  p_45to49in=V066,
  p_45to49=V062 + V063 + V064 + V065 + V066,
  p_50to54wh=V067,
  p_50to54bl=V068,
  p_50to54ye=V069,
  p_50to54br=V070,
  p_50to54in=V071,
  p_50to54=V067 + V068 + V069 + V070 + V071,
  p_55to59wh=V072,
  p_55to59bl=V073,
  p_55to59ye=V074,
  p_55to59br=V075,
  p_55to59in=V076,
  p_55to59=V072 + V073 + V074 + V075 + V076,
  p_60to69wh=V077,
  p_60to69bl=V078,
  p_60to69ye=V079,
  p_60to69br=V080,
  p_60to69in=V081,
  p_60to69=V077 + V078 + V079 + V080 + V081,
  p_70mwh=V082,
  p_70mbl=V083,
  p_70mye=V084,
  p_70mbr=V085,
  p_70min=V086,
  p_70m=V082 + V083 + V084 + V085 + V086,
  p_5or6wh_m=V087,
  p_5or6bl_m=V088,
  p_5or6ye_m=V089,
  p_5or6br_m=V090,
  p_5or6in_m=V091,
  p_5or6=V087 + V088 + V089 + V090 + V091,
  p_7to9wh_m=V092,
  p_7to9bl_m=V093,
  p_7to9ye_m=V094,
  p_7to9br_m=V095,
  p_7to9in_m=V096,
  p_7to9_m=V092 + V093 + V094 + V095 + V096,
  p_10to14wh_m=V097,
  p_10to14bl_m=V098,
  p_10to14ye_m=V099,
  p_10to14br_m=V100,
  p_10to14in_m=V101,
  p_10to14_m=V097 + V098 + V099 + V100 +  V101,
  p_15to19wh_m=V102,
  p_15to19bl_m=V103,
  p_15to19ye_m=V104,
  p_15to19br_m=V105,
  p_15to19in_m=V106,
  p_15to19_m=V102 + V103 + V104 + V105 + V106,
  p_15to17wh_m=V107,
  p_15to17bl_m=V108,
  p_15to17ye_m=V109,
  p_15to17br_m=V110,
  p_15to17in_m=V111,
  p_15to17_m=V107 + V108 + V109 + V110 + V111,
  p_18or19wh_m=V112,
  p_18or19bl_m=V113,
  p_18or19ye_m=V114,
  p_18or19br_m=V115,
  p_18or19in_m=V116,
  p_18or19_m=V112 + V113 + V114 + V115 + V116,
  p_20to24wh_m=V117,
  p_20to24bl_m=V118,
  p_20to24ye_m=V119,
  p_20to24br_m=V120,
  p_20to24in_m=V121,
  p_20to24_m=V117 + V118 + V119 + V120 + V121,
  p_25to29wh_m=V122,
  p_25to29bl_m=V123,
  p_25to29ye_m=V124,
  p_25to29br_m=V125,
  p_25to29in_m=V126,
  p_25to29_m=V122 + V123 + V124 + V125 + V126,
  p_30to34wh_m=V127,
  p_30to34bl_m=V128,
  p_30to34ye_m=V129,
  p_30to34br_m=V130,
  p_30to34in_m=V131,
  p_30to34_m=V127 + V128 + V129 + V130 + V131,
  p_35to39wh_m=V132,
  p_35to39bl_m=V133,
  p_35to39ye_m=V134,
  p_35to39br_m=V135,
  p_35to39in_m=V136,
  p_35to39_m=V132 + V133 + V134 + V135 + V136,
  p_40to44wh_m=V137,
  p_40to44bl_m=V138,
  p_40to44ye_m=V139,
  p_40to44br_m=V140,
  p_40to44in_m=V141,
  p_40to44_m=V137 + V138 + V139 + V140 + V141,
  p_45to49wh_m=V142,
  p_45to49bl_m=V143,
  p_45to49ye_m=V144,
  p_45to49br_m=V145,
  p_45to49in_m=V146,
  p_45to49_m=V142 + V143 + V144 + V145 + V146 ,
  p_50to54wh_m=V147,
  p_50to54bl_m=V148,
  p_50to54ye_m=V149,
  p_50to54br_m=V150,
  p_50to54in_m=V151,
  p_50to54_m=V147 + V148 + V149 + V150 + V151 ,
  p_55to59wh_m=V152,
  p_55to59bl_m=V153,
  p_55to59ye_m=V154,
  p_55to59br_m=V155,
  p_55to59in_m=V156,
  p_55to59_m=V152 + V153 + V154 + V155 + V156,
  p_60to69wh_m=V157,
  p_60to69bl_m=V158,
  p_60to69ye_m=V159,
  p_60to69br_m=V160,
  p_60to69in_m=V161,
  p_60to69_m=V157 + V158 + V159 + V160 + V161,
  p_70mwh_m=V162,
  p_70mbl_m=V163,
  p_70mye_m=V164,
  p_70mbr_m=V165,
  p_70min_m=V166,
  p_70m_m=V162 + V163 + V164 + V165 + V166,
  p_5or6wh_f=V167,
  p_5or6bl_f=V168,
  p_5or6ye_f=V169,
  p_5or6br_f=V170,
  p_5or6in_f=V171,
  p_5or6_f=V167 + V168 + V169 + V170 + V171 ,
  p_7to9wh_f=V172,
  p_7to9bl_f=V173,
  p_7to9ye_f=V174,
  p_7to9br_f=V175,
  p_7to9in_f=V176,
  p_7to9_f=V172 +V173 + V174 + V175 + V176,
  p_10to14wh_f=V177,
  p_10to14bl_f=V178,
  p_10to14ye_f=V179,
  p_10to14br_f=V180,
  p_10to14in_f=V181,
  p_10to14_f=V177 + V178 + V178 + V179 + V180 + V181,
  p_15to19wh_f=V182,
  p_15to19bl_f=V183,
  p_15to19ye_f=V184,
  p_15to19br_f=V185,
  p_15to19in_f=V186,
  p_15to19_f=V182 + V183 + V184 + V185 + V186,
  p_15to17wh_f=V187,
  p_15to17bl_f=V188,
  p_15to17ye_f=V189,
  p_15to17br_f=V190,
  p_15to17in_f=V191,
  p_15to17_f=V187 + V188 + V189 + V190 + V191,
  p_18or19wh_f=V192,
  p_18or19bl_f=V193,
  p_18or19ye_f=V194,
  p_18or19br_f=V195,
  p_18or19in_f=V196,
  p_18or19_f=V192 + V193 + V194 + V195 + V196,
  p_20to24wh_f=V197,
  p_20to24bl_f=V198,
  p_20to24ye_f=V199,
  p_20to24br_f=V200,
  p_20to24in_f=V201,
  p_20to24_f=V197 + V198 + V199 + V200 + V201,
  p_25to29wh_f=V202,
  p_25to29bl_f=V203,
  p_25to29ye_f=V204,
  p_25to29br_f=V205,
  p_25to29in_f=V206,
  p_25to29_f=V202 + V203 + V204 + V205 + V206,
  p_30to34wh_f=V207,
  p_30to34bl_f=V208,
  p_30to34ye_f=V209,
  p_30to34br_f=V210,
  p_30to34in_f=V211,
  p_30to34_f=V207 + V208 + V209 + V210 + V211,
  p_35to39wh_f=V212,
  p_35to39bl_f=V213,
  p_35to39ye_f=V214,
  p_35to39br_f=V215,
  p_35to39in_f=V216,
  p_35to39_f=V212 + V213 + V214 + V215 + V216,
  p_40to44wh_f=V217,
  p_40to44bl_f=V218,
  p_40to44ye_f=V219,
  p_40to44br_f=V220,
  p_40to44in_f=V221,
  p_40to44_f=V217 + V218 + V219 + V220 + V221,
  p_45to49wh_f=V222,
  p_45to49bl_f=V223,
  p_45to49ye_f=V224,
  p_45to49br_f=V225,
  p_45to49in_f=V226,
  p_45to49_f=V222 + V223 + V224 + V225 + V226,
  p_50to54wh_f=V227,
  p_50to54bl_f=V228,
  p_50to54ye_f=V229,
  p_50to54br_f=V230,
  p_50to54in_f=V231,
  p_50to54_f=V227 + V228 + V229 + V230 + V231,
  p_55to59wh_f=V232,
  p_55to59bl_f=V233,
  p_55to59ye_f=V234,
  p_55to59br_f=V235,
  p_55to59in_f=V236,
  p_55to59_f=V232 + V233 + V234 + V235 + V236,
  p_60to69wh_f=V237,
  p_60to69bl_f=V238,
  p_60to69ye_f=V239,
  p_60to69br_f=V240,
  p_60to69in_f=V241,
  p_60to69_f=V237 + V238 + V239 + V240 + V241,
  p_70mwh_f=V242,
  p_70mbl_f=V243,
  p_70mye_f=V244,
  p_70mbr_f=V245,
  p_70min_f=V246,
  p_70m_f=V242 + V243 + V244 + V245 + V246 ,

  p_0to14=p_0to4   + p_5to9 + p_10to14,
  p_15to69=p_tot - p_0to14 - p_70m,
  p_prop_tdr=(p_0to14 + p_70m) / p_15to69 *100,
  p_prop_chddr=p_0to14  / p_15to69 *100,
  p_prop_olddr= p_70m / p_15to69 *100
  
)

pessoa03 <- select(pessoa03, 
                   254:558
                   )
#write_xlsx(pessoa03, "01_data/02_proc/pessoa03.xlsx")

df_1 <- full_join(pessoa03, domicilio01)


# basico ------------------------------------------------------------------

# Set the directory path
directory_path <- "01_data/01_raw/sc/01_xls_topic/Basico"

# Get a list of XLS files in the directory
file_list <- list.files(directory_path, pattern = "\\.xls$", full.names = TRUE)

# Initialize an empty list to store data frames
data_list <- list()

# Loop through the list of files and read them into data frames
for (file in file_list) {
  data <- read_excel(file)
  data_list[[file]] <- data
}

# Combine all data frames into one
basico <- do.call(rbind, data_list)


basico <- basico %>% mutate(
  tract_id = Cod_setor,
  d_mean_occup=V003,
  d_var_occup=V004,
  hh_mean_inc1=V005,
  hh_var_inc1=V006,
  hh_mean_inc2=V007,
  hh_var_inc2=V008,
  p_mean_inc1=V009,
  p_var_inc1=V010,
  p_mean_inc2=V011,
  p_var_inc2=V012
  )

basico <- select(basico, 
                   34:44)


df_2 <- full_join(basico, df_1)

fwrite(df_2, "01_data/02_proc/02_indicators/ct.csv")

# split by reg_code -----------------------------------------------------
dir.create("01_data/02_proc/02_indicators/sc")
output_dir <- "01_data/02_proc/02_indicators/sc"
# Split the dataframe by reg
df_list <- split(df_2, df_2$reg_code)

# Save each subset as a CSV file
for (code_reg_value in names(df_list)) {
  subset_df <- df_list[[code_reg_value]]
  file_path <- file.path(output_dir, paste0(code_reg_value, ".csv"))
  
  write.csv(subset_df, file_path, row.names = FALSE, fileEncoding = "latin1")
}

```

We now have the tabular data by geographical level prepared and ready for use. In the next section, we will calculate the WRI Urban Inclusion Index^[For detailed information on the conceptualization and methods, please consult:].

# WRI Urban Inclusion Index (IISU) `bra_03_iisu.R`
The code calculates the WRI Urban Inclusion Index (WRI-IISU) for the weigthing areas by deriving a composite index through Principal Component Analysis (PCA) which is applied to selected indicators to create a composite index (PC1) at both the national and metropolitan levels. These indices are rescaled and categorized into groups ranging from "Very low" to "Very high" using natural breaks classification.

Next, the script loops through each metropolitan area, performing PCA and generating indices for each area. It saves the results, including the index values and PCA loadings, into separate directories. The script also includes a function to read and combine multiple *CSV* files, allowing for the aggregation of results across different metropolitan areas. Finally, the script merges the metropolitan data with the original dataset, creating a comprehensive dataset that includes both the national and metropolitan indices, and saves it as a CSV file.
```{r}
rm(list=ls()) # Clear environment
library(tidyverse)
library(data.table)
library(reshape2)
library(pander)
library(ggplot2)
library(DT)
library(stringi)
library(RcmdrMisc)
library(ggcorrplot)
library(gridExtra)
library(psych)
library(classInt)

data_format <- function(df) {
  df <- df %>%
    mutate(across(ends_with("_id"), as.character)) %>%
    mutate(across(ends_with("_code"), as.character)) 
  return(df)
}

vars <- c(
  #'d_prop_walls_masonry',
            'd_prop_bthrm','d_prop_water_netwk',
            #"d_prop_novc",
            'd_prop_garbage_clngsvcs',
            'd_prop_elect',
            'd_prop_inet',
            'd_prop_tv',
            'd_prop_wm','d_prop_refri',
            'd_prop_cp',
            'd_prop_cmp',
            'd_mean_pcincome',
            #'p_prop_6to11as', 
            #'p_prop_12to17as', 
            'p_prop_18to24as'
            #'p_prop_emp'
)

df <- fread("01_data/02_proc/02_indicators/wa.csv")

# Convert reg_code to a factor
df$reg_code <- as.factor(df$reg_code)

# Format uf_code, mun_code, and mesoreg_code with leading zeros
df <- df %>%
  mutate(
    uf_code = sprintf("%02d", uf_code),
    mun_code = sprintf("%05d", mun_code),
    mesoreg_code = sprintf("%02d", mesoreg_code),
    microreg_code = sprintf("%03d", microreg_code),
    wa_id = sprintf("%013s", wa_id),
    mun_id = paste0(uf_code, mun_code)
  )

# Apply additional formatting using the data_format function
df <- data_format(df)
df <- data_format(df)

df[, (vars) := lapply(.SD, function(x) ifelse(is.na(x), 0, x)), .SDcols = vars]

pca_nat <- prcomp(select(df, all_of(vars)), scale = TRUE)
print(pca_nat)
summary(pca_nat)

PC <- as.data.frame(pca_nat[2])
PC1_s  <- nrow(PC[PC$rotation.PC1<0,]) # to identify negative charge
pred <- predict(pca_nat, newdata = df) 
pred <- as.data.frame(pred)

df$iisu_nat <- pred$PC1
df$iisu_nat_s <- rescale_1(df$iisu_nat)

# Determine natural breaks
breaks <- classIntervals(df$iisu_nat_s, n = 5, style = "jenks")$brks

# Assign strata based on natural breaks
df$iisu_nat_g <- cut(df$iisu_nat_s, breaks = breaks, include.lowest = TRUE, labels = c("Very low", "Low", "Medium", "High", "Very high"))

# Display the distribution
table(df$iisu_nat_g)
prop.table(table(df$iisu_nat_g))*100

# Metropolitan - between
vars <- c(
  #'d_prop_walls_masonry',
            'd_prop_bthrm',
            #'d_prop_water_netwk',
            #"d_prop_novc",
            'd_prop_garbage_clngsvcs',
            #'d_prop_elect',
            'd_prop_inet',
            'd_prop_tv',
            'd_prop_wm','d_prop_refri',
            'd_prop_cp',
            'd_prop_cmp',
            'd_mean_pcincome',
            #'p_prop_6to11as', 
            #'p_prop_12to17as', 
            'p_prop_18to24as'
            #'p_prop_emp'
)
df_metro1 <- df %>% filter(!is.na(metro_name))

pca_natmetro <- prcomp(select(df_metro1, all_of(vars)), scale = TRUE)
print(pca_natmetro)
summary(pca_natmetro)

PC <- as.data.frame(pca_natmetro[2])
PC1_s  <- nrow(PC[PC$rotation.PC1<0,]) # to identify negative charge
pred <- predict(pca_natmetro, newdata = df_metro1) 
pred <- as.data.frame(pred)

df_metro1$iisu_natmetro <- pred$PC1
df_metro1$iisu_natmetro_s <- rescale_1(df_metro1$iisu_natmetro)

breaks <- classIntervals(df_metro1$iisu_nat_s, n = 5, style = "jenks")$brks


df_metro1$iisu_natmetro_g <- cut(df_metro1$iisu_natmetro_s, breaks = breaks, include.lowest = TRUE, labels = c("Very low", "Low", "Medium", "High", "Very high"))

table(df_metro1$iisu_natmetro_g)
prop.table(table(df_metro1$iisu_natmetro_g))*100

# Metropolitan within
# Create directories 
dir.create("01_data/02_proc/03_iisu/metro", showWarnings = FALSE)
dir.create("01_data/02_proc/03_iisu/metro/index", showWarnings = FALSE)
dir.create("01_data/02_proc/03_iisu/metro/PCA", showWarnings = FALSE)
dir_path = "01_data/02_proc/03_iisu/metro/index/"
dir_path_2 = "01_data/02_proc/03_iisu/metro/PCA/"

sanitize_file_name <- function(name) {
  name %>%
    gsub("[/\\:*?\"<>|]", "_", .) %>%  
    gsub("\\s+", "_", .) %>%           
    gsub("\\(", "", .) %>%             
    gsub("\\)", "", .)                
}

# Filter metros with more than 29 unique wa_id
metros <- df %>%
  group_by(metro_name) %>%
  summarise(unique_wa_id_count = n_distinct(wa_id)) %>%
  filter(unique_wa_id_count > 29) %>%
  pull(metro_name)

for (i in 1:length(metros)) {
  
  metro_i = metros[i]
  
  df_2 = df %>% 
    filter(metro_name == metro_i) %>%
    drop_na(all_of(vars))  # Omit rows with NAs in the relevant columns
  
  if (nrow(df_2) == 0) {
    next  # Skip the rest of the loop if df_2 is empty after omitting NAs
  }
  
  pca_metro <- prcomp(select(df_2, all_of(vars)), scale = TRUE)

  PC <- as.data.frame(pca_metro$rotation[, 1:3])
  PC1_s  <- nrow(PC[PC$rotation.PC1 < 0,])
  
  pred <- predict(pca_metro, newdata = df_2)
  pred <- as.data.frame(pred)
  
  df_2$iisu_metro <- pred$PC1

  df_2$iisu_metro_s <- if (PC1_s == 0) {
    rescale_1(df_2$iisu_metro)
  } else {
    rescale_2(df_2$iisu_metro)
  }

  # Add a small jitter to ensure unique breaks
  df_2$iisu_metro_s <- jitter(df_2$iisu_metro_s)

  breaks <- classIntervals(df_2$iisu_metro_s, n = 5, style = "jenks")$brks
  
  if (length(unique(breaks)) < length(breaks)) {
    breaks <- unique(breaks)
  }
  
  if (length(breaks) != 6) {
    breaks <- classIntervals(df_2$iisu_metro_s, n = 5, style = "equal")$brks
  }

  df_2$iisu_metro_g <- cut(df_2$iisu_metro_s, breaks = breaks, include.lowest = TRUE, labels = c("Very low", "Low", "Medium", "High", "Very high"))
  
  PC <- as.data.frame(pca_metro$rotation[, 1:3])
  colnames(PC) <- paste0("PC_", 1:3)
  PC$indicators <- rownames(pca_metro$rotation)
  PC$metro <- metro_i
  
  PC <- select(PC, metro, indicators, PC_1, PC_2, PC_3)

  # Sanitize metro_i for file names
  metro_i_sanitized <- sanitize_file_name(metro_i)

  write.csv(df_2,
            file = file.path(dir_path, paste0(metro_i_sanitized, ".csv")),
            fileEncoding = "latin1", row.names = FALSE)
            
  write.csv(PC,
            file = file.path(dir_path_2, paste0(metro_i_sanitized, ".csv")),
            fileEncoding = "latin1", row.names = FALSE)
}

# Define the custom function
read_all_csv <- function(base_dir) {
  # Get a list of all CSV files in the directory
  file_list <- list.files(base_dir, pattern = "\\.csv$", full.names = TRUE)
  
  # Read each CSV file and store in a list
   df_list <- lapply(file_list, function(file) read.csv(file, fileEncoding = "latin1"))
  
  # Find the union of all column names
  all_columns <- Reduce(union, lapply(df_list, colnames))
  
  # Ensure all data frames have the same columns
  df_list <- lapply(df_list, function(df) {
    df <- df %>% select(all_of(all_columns), everything())
    df[setdiff(all_columns, colnames(df))] <- NA
    return(df)
  })
  
  # Combine all data frames into one
  combined_df <- bind_rows(df_list)
  
  return(combined_df)
}

# Use the function to read all CSV files
base_dir <- "01_data/02_proc/03_iisu/metro/index"
df_metro <- read_all_csv(base_dir)

df_metro$iisu_metro_g <- factor(df_metro$iisu_metro_g,
                                levels = c("Very low", "Low", "Medium", "High", "Very high"),
                                labels = c("Very low", "Low", "Medium", "High", "Very high")
                                )


df_metro1 <- select(df_metro1,
                   wa_id,iisu_natmetro, iisu_natmetro_s, iisu_natmetro_g
                   )

df_metro1$wa_id <- as.character(df_metro1$wa_id)

df_metro <- select(df_metro,
                   wa_id,iisu_metro, iisu_metro_s, iisu_metro_g
                   )

df_metro$wa_id <- as.character(df_metro$wa_id)

df <- full_join(df, df_metro1, by="wa_id")

df_final <- full_join(df, df_metro, by="wa_id")

write.csv(df_final, "01_data/02_proc/02_indicators/wa_index.csv", 
      row.names = FALSE, fileEncoding="latin1")
```

# Geojoin `bra_04_geojoin.R`
Finally, the following code processes and prepares geographic and tabular data for all the geographical leves. The code reads in *CSV* files containing tabular for each geographic level, formats them, and joins them with geographic boundary data stored in GeoJSON files. The boundaries are processed using the sf package, which allows for spatial data manipulation.

For each geographic level the code:

* Data Formatting: Reads and formats the indicator data, ensuring consistency in IDs, adding necessary columns like year and entity_level (necessary for the dashboard), and removing unnecessary columns.
* Geographic Join: Merges the tabular data with the corresponding geographic boundary data based on unique identifiers (like mun_id, wa_id, and tract_id).

* Directory Management: Creates directories to store the processed data, organizing the results by regional codes (reg_code).

* Output: Writes the final merged data (containing both geographic and tabular information) into **GeoJSON** files, saving them in the appropriate directories.

```{r}
rm(list=ls()) # Clear environment

library(data.table)
library(tidyverse)
library(dplyr)
library(sf)

data_format <- function(df) {
  df <- df |>
    mutate(across(ends_with("_id"), as.character)) |>
    mutate(across(ends_with("_code"), as.character)) 
  return(df)
}

columns_to_add <- c(
  "reg_code","reg_name",  
  "uf_code", "uf_name",
  "mesoreg_code","mesoreg_id","mesoreg_name",
  "microreg_code","microreg_id",
  "metro_name", "metro_type",
  "mun_code", "mun_id", "mun_name",  
  "wa_id", "tract_id"
)
dir.create("01_data/02_proc/04_census_geo")

# mapping -----------------------------------------------------------------
# Read data
mapping <- fread("01_data/01_raw/boundaries/geo_levels_names.csv",
                 encoding = "Latin-1")

# Extract codes from ids
mapping <- mapping |>
  mutate(
    uf_code = substr(mun_id, 1, 2),
    mun_code = substr(mun_id, 3, 7),
    mesoreg_code = substr(mesoreg_id, 3, 4),
    microreg_code = substr(microreg_id, 3, 5)) |>
  data_format()

# nat ---------------------------------------------------------------------
# Create necessary directories
dir.create("01_data/02_proc/04_census_geo/nat/All", recursive = TRUE, showWarnings = FALSE)

# Load data and set additional columns
df <- fread("01_data/02_proc/02_indicators/nat.csv")
df <- df |>
  mutate(year = "2010", entity_level = "Nation")

# Read geojson and join with data
geo <- st_read("01_data/01_raw/boundaries/nat.geojson")
census_geo <- cbind(df, geo, all = TRUE)

param_cols <- setdiff(columns_to_add, colnames(census_geo))

if(length(param_cols) > 0) {
  # Generate missing columns
  census_geo[, param_cols] <- NA
}

# Write the result to a geojson file
st_write(census_geo, "01_data/02_proc/04_census_geo/nat/All/census_geo.geojson", append = FALSE)

# region ------------------------------------------------------------------
# Load indicator data and add columns for year and entity level
df <- fread("01_data/02_proc/02_indicators/reg.csv") |>
  mutate(year = "2010", entity_level = "Region")

# Read the regional boundaries from the GeoJSON file
geo <- st_read("01_data/01_raw/boundaries/reg.geojson")

# Perform a full join of the indicator data and geographic boundaries by region code
census_geo <- full_join(df, geo, by = "reg_code")

# Identify columns that need to be added to the dataset
param_cols <- setdiff(columns_to_add, colnames(census_geo))

if(length(param_cols) > 0) {
  # Generate missing columns
  census_geo[, param_cols] <- NA
}

# Create necessary directories for storing processed data
dir.create("01_data/02_proc/04_census_geo/reg/All", recursive = TRUE, showWarnings = FALSE)

# Save the complete dataset as a GeoJSON file
st_write(census_geo, "01_data/02_proc/04_census_geo/reg/All/census_geo.geojson", append = FALSE)

# Get a list of unique region codes
reg_codes <- unique(census_geo$reg_code)

# Loop through each region code to create directories and save GeoJSON files
for (reg_i_code in reg_codes) {
  
  # Filter the dataset for the current region
  census_geo_i <- census_geo |>
    filter(reg_code == reg_i_code)
  
  # Define the directory path for the current region and create the directory
  dir_path <- file.path("01_data/02_proc/04_census_geo/reg", reg_i_code)
  dir.create(dir_path, recursive = TRUE, showWarnings = FALSE)
  
  # Define the file path for the GeoJSON file and save the filtered dataset
  file_path <- file.path(dir_path, "census_geo.geojson")
  st_write(census_geo_i, file_path, delete_dsn = TRUE, append = FALSE)
}
# uf ----------------------------------------------------------------
# Load indicator data and apply custom formatting
df <- fread("01_data/02_proc/02_indicators/uf.csv") |>
  data_format() |>  # Apply the data_format function to clean or format the data
  mutate(year = "2010", entity_level = "Federation Unit")  # Add year and entity level columns

# Read the geographic boundaries for Federation Units and apply custom formatting
geo <- st_read("01_data/01_raw/boundaries/uf.geojson") |>
  data_format()  # Apply the data_format function to clean or format the geographic data

# Perform a full join of the indicator data and geographic boundaries by region and UF codes
census_geo <- full_join(df, geo, by = c("reg_code", "uf_code"))

param_cols <- setdiff(columns_to_add, colnames(census_geo))
if(length(param_cols) > 0) {
  census_geo[, param_cols] <- NA
}

dir.create("01_data/02_proc/04_census_geo/uf/All", recursive = TRUE, showWarnings = FALSE)
st_write(census_geo, "01_data/02_proc/04_census_geo/uf/All/census_geo.geojson")

reg_codes <- unique(census_geo$reg_code)
# Create directory for each department
for (reg_i_code in reg_codes) {
  census_geo_i <- census_geo |>
    filter(reg_code == reg_i_code)
  dir_path <- file.path("./01_data/02_proc/04_census_geo/uf/", reg_i_code)
  dir.create(dir_path, recursive = TRUE, showWarnings = FALSE)
  file_path <- file.path(dir_path, "census_geo.geojson")
  st_write(census_geo_i, file_path, delete_dsn = TRUE, append=FALSE)
}

# mesoreg ----------------------------------------------------------------------
df <- fread("01_data/02_proc/02_indicators/mesoreg.csv") |>
  mutate(mesoreg_code = str_pad(mesoreg_code, width = 2, pad = "0"),
         mesoreg_id = paste0(uf_code, mesoreg_code))

geo <- st_read("01_data/01_raw/boundaries/mesoreg.geojson") |>
  data_format()

mapping_s <- mapping |>
  select(reg_name, mesoreg_id) |>
  distinct()

df <- df |>
  left_join(mapping_s, by = "mesoreg_id") |>
  data_format() |>
  mutate(year = "2010", entity_level = "Mesoregion")

census_geo <- full_join(df, geo, by = c("uf_code", "mesoreg_id"))

param_cols <- setdiff(columns_to_add, colnames(census_geo))
if(length(param_cols) > 0) {
  census_geo[, param_cols] <- NA
}

dir.create("01_data/02_proc/04_census_geo/mesoreg/All", recursive = TRUE, showWarnings = FALSE)
st_write(census_geo, "01_data/02_proc/04_census_geo/mesoreg/All/census_geo.geojson")
                       
reg_codes <- unique(census_geo$reg_code)
# Create directory for each region
  for (reg_i_code in reg_codes) {
    census_geo_i <- census_geo |>
    filter(reg_code == reg_i_code)
    dir_path <- file.path("./01_data/02_proc/04_census_geo/mesoreg/", reg_i_code)
    dir.create(dir_path, recursive = TRUE, showWarnings = FALSE)
    file_path <- file.path(dir_path, "census_geo.geojson")
                         st_write(census_geo_i, file_path, delete_dsn = TRUE, append=FALSE)
                       }
# microreg ----------------------------------------------------------------
# Load and format indicator data for microregions
df <- fread("01_data/02_proc/02_indicators/microreg.csv") |>
  data_format() |>  # Apply custom data formatting
  mutate(
    mesoreg_code = str_pad(mesoreg_code, width = 2, pad = "0"),  # Pad mesoreg_code to 2 digits
    microreg_code = str_pad(microreg_code, width = 3, pad = "0"),  # Pad microreg_code to 3 digits
    microreg_id = paste0(uf_code, microreg_code),  # Create microreg_id by concatenating uf_code and microreg_code
    year = "2010",  # Add year column
    entity_level = "Microregion"  # Add entity level column
  )

# Read and format geographic boundaries for microregions
geo <- st_read("01_data/01_raw/boundaries/microreg.geojson") |>
  data_format()  # Apply custom data formatting

# Select relevant columns from mapping and remove duplicates
mapping_s <- mapping |>
  select(reg_name, mesoreg_id, microreg_id) |>
  distinct()  # Remove duplicated rows

# Merge the formatted indicator data with the selected mapping data
df <- df |>
  left_join(mapping_s, by = "microreg_id") |>
  data_format()  # Apply additional data formatting

# Perform a full join of the indicator data and geographic boundaries by UF code and microregion ID
census_geo <- full_join(df, geo, by = c("uf_code", "microreg_id"))
param_cols <- setdiff(columns_to_add, colnames(census_geo))
if(length(param_cols) > 0) {
  # Generate missing columns
  census_geo[, param_cols] <- NA
}

dir.create("01_data/02_proc/04_census_geo/microreg/All", recursive = TRUE, showWarnings = FALSE)
st_write(census_geo, "01_data/02_proc/04_census_geo/microreg/All/census_geo.geojson")

reg_codes <- unique(census_geo$reg_code)
# Create directory for each department
for (reg_i_code in reg_codes) {
  census_geo_i <- census_geo |>
    filter(reg_code == reg_i_code)
  dir_path <- file.path("./01_data/02_proc/04_census_geo/microreg/", reg_i_code)
  dir.create(dir_path, recursive = TRUE, showWarnings = FALSE)
  file_path <- file.path(dir_path, "census_geo.geojson")
  st_write(census_geo_i, file_path, delete_dsn = TRUE, append=FALSE)
}

# mun ---------------------------------------------------------------------
# Load and format indicator data for municipalities
df <- fread("01_data/02_proc/02_indicators/mun.csv") |>
  data_format() |>  # Apply custom data formatting
  mutate(
    mun_code = str_pad(mun_code, width = 5, pad = "0"),  # Pad mun_code to 5 digits
    mun_id = paste0(uf_code, mun_code),  # Create mun_id by concatenating uf_code and mun_code
    year = "2010",  # Add year column
    entity_level = "Municipality"  # Add entity level column
  ) |>
  select(-reg_code, -uf_code, -mesoreg_code, -microreg_code, -metro_code, -mun_code)  # Drop unnecessary columns

# Read and format geographic boundaries for municipalities
geo <- st_read("01_data/01_raw/boundaries/mun.geojson") |>
  data_format()  # Apply custom data formatting

# Merge the formatted indicator data with the mapping data
df <- df |>
  left_join(mapping, by = "mun_id") |>
  data_format()  # Apply additional data formatting

# Perform a full join of the indicator data and geographic boundaries by UF code and municipality ID
census_geo <- full_join(df, geo, by = c("uf_code", "mun_id"))

param_cols <- setdiff(columns_to_add, colnames(census_geo))
if(length(param_cols) > 0) {
  # Generate missing columns
  census_geo[, param_cols] <- NA
}

# Create directories for storing processed data
dir.create("01_data/02_proc/04_census_geo/mun/All", recursive = TRUE, showWarnings = FALSE)

st_write(census_geo, "01_data/02_proc/04_census_geo/mun/All/census_geo.geojson")

reg_codes <- unique(census_geo$reg_code)
# Create directory for each department
for (reg_i_code in reg_codes) {
  census_geo_i <- census_geo |>
    filter(reg_code == reg_i_code)
  dir_path <- file.path("./01_data/02_proc/04_census_geo/mun/", reg_i_code)
  dir.create(dir_path, recursive = TRUE, showWarnings = FALSE)
  file_path <- file.path(dir_path, "census_geo.geojson")
  st_write(census_geo_i, file_path, delete_dsn = TRUE, append=FALSE)
}
# wa ---------------------------------------------------------------------
# Load and format weighting area indicator data, removing unnecessary columns
df <- fread("01_data/02_proc/02_indicators/wa.csv") |>
  data_format() |>
  select(-metro_code, -metro_name, -uf_code, -mun_code, -reg_code, -mesoreg_code, -microreg_code)

# Merge the formatted indicator data with the mapping data by 'mun_id'
df <- full_join(mapping, df, by = "mun_id") |>
  mutate(
    year = "2010",  # Add year column
    entity_level = "Weighting Area"  # Add entity level column
  )

# Read and format geographic boundaries for weighting areas
geo <- st_read("01_data/01_raw/boundaries/wa.geojson") |>
  data_format() |>
  select(code_weighting, geometry) |> 
  rename(wa_id = code_weighting)

# Perform a full join of the indicator data and geographic boundaries by 'wa_id'
census_geo <- full_join(df, geo, by = "wa_id")
param_cols <- setdiff(columns_to_add, colnames(census_geo))
if(length(param_cols) > 0) {
  # Generate missing columns
  census_geo[, param_cols] <- NA
}

dir.create("01_data/02_proc/04_census_geo/wa/All", recursive = TRUE, showWarnings = FALSE)
st_write(census_geo, "01_data/02_proc/04_census_geo/wa/All/census_geo.geojson")

reg_codes <- unique(census_geo$reg_code)
# Create directory for each department
for (reg_i_code in reg_codes) {
  census_geo_i <- census_geo |>
    filter(reg_code == reg_i_code)
  dir_path <- file.path("./01_data/02_proc/04_census_geo/wa/", reg_i_code)
  dir.create(dir_path, recursive = TRUE, showWarnings = FALSE)
  file_path <- file.path(dir_path, "census_geo.geojson")
  st_write(census_geo_i, file_path, delete_dsn = TRUE, append=FALSE)
}
# ct ----------------------------------------------------------------------
# Load and process the census tract indicator data
df <- fread("01_data/02_proc/02_indicators/ct.csv") |>
  mutate(
    year = "2010",                        # Add year column
    entity_level = "Census tract"         # Add entity level column
  ) |> 
  data_format()

# Load and process the geographic boundaries for census tracts
geo <- st_read("01_data/01_raw/boundaries/tract.geojson") |>
  data_format()

# Merge the indicator data and geographic boundaries by 'tract_id'
census_geo <- full_join(df, geo, by = "tract_id") |>
  data_format()  # Apply final formatting

census_geo <- full_join(mapping, census_geo, by = "mun_id") 
names(census_geo)
param_cols <- setdiff(columns_to_add, colnames(census_geo))
if(length(param_cols) > 0) {
  census_geo[, param_cols] <- NA
}

dir.create("01_data/02_proc/04_census_geo/ct/All",  recursive = TRUE, showWarnings = FALSE)
st_write(census_geo, "01_data/02_proc/04_census_geo/ct/All/census_geo.geojson")

reg_codes <- unique(census_geo$reg_code)
# Create directory for each department
for (reg_i_code in reg_codes) {
  census_geo_i <- census_geo |>
    filter(reg_code == reg_i_code)
  dir_path <- file.path("./01_data/02_proc/04_census_geo/ct/", reg_i_code)
  dir.create(dir_path, recursive = TRUE, showWarnings = FALSE)
  file_path <- file.path(dir_path, "census_geo.geojson")
  st_write(census_geo_i, file_path, delete_dsn = TRUE, append=FALSE)
}
```

The result is a set of GeoJSON files that can be used for spatial analysis or visualization, with each file containing both the geographic boundaries and associated indicators for the 2010 Census. The data is organized by geographic level and region, making it easier to manage and analyze on a regional basis.
